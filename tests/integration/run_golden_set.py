#!/usr/bin/env python3
"""
è¿è¡Œé»„é‡‘æµ‹è¯•é›†

é»„é‡‘æµ‹è¯•é›†æ˜¯ç”¨äºABæµ‹è¯•å’Œæ¨¡å‹å¯¹æ¯”çš„æ ‡å‡†æµ‹è¯•é›†
åŒ…å«50ä¸ªæœ€å…·ä»£è¡¨æ€§çš„æµ‹è¯•ç”¨ä¾‹
"""

import asyncio
import sys
import argparse
from pathlib import Path
from datetime import datetime
import yaml

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from base_new import IntegrationTestBase
from reporters import SingleRunReporter


class GoldenSetRunner(IntegrationTestBase):
    """é»„é‡‘æµ‹è¯•é›†è¿è¡Œå™¨"""
    
    def __init__(self, test_cases_file: Path):
        super().__init__(test_suite_name="golden_set")
        self.test_cases_file = test_cases_file
        self.test_cases = []
        
    def load_test_cases(self):
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        with open(self.test_cases_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        self.test_cases = []
        for category_name, cases in data.items():
            if isinstance(cases, list) and category_name not in ['test_suite_name', 'test_suite_version', 'total_cases']:
                self.test_cases.extend(cases)
        
        print(f"âœ… åŠ è½½äº† {len(self.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return len(self.test_cases)
    
    async def run_all_tests(self, limit: int = None):
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        
        Args:
            limit: é™åˆ¶è¿è¡Œçš„ç”¨ä¾‹æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        """
        cases_to_run = self.test_cases[:limit] if limit else self.test_cases
        
        print(f"\nå¼€å§‹è¿è¡Œ {len(cases_to_run)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
        print("=" * 80)
        
        for i, test_case in enumerate(cases_to_run, 1):
            print(f"\n[{i}/{len(cases_to_run)}] è¿è¡Œä¸­...")
            
            await self.run_test(
                test_id=test_case['test_id'],
                test_name=test_case['test_name'],
                message=test_case['user_input'],
                expected_behavior=test_case['expected_behavior'],
                data_verification=test_case.get('data_verification'),
                intelligence_check=test_case.get('intelligence_check'),
                experience_check=test_case.get('experience_check')
            )
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡å¿«
            await asyncio.sleep(0.5)


async def main():
    parser = argparse.ArgumentParser(description='è¿è¡ŒFAAé»„é‡‘æµ‹è¯•é›†')
    parser.add_argument('--config', type=str, help='é…ç½®å‚æ•°ï¼ˆJSONæ ¼å¼ï¼‰')
    parser.add_argument('--limit', type=int, help='é™åˆ¶è¿è¡Œçš„ç”¨ä¾‹æ•°é‡')
    parser.add_argument('--output-dir', type=str, default='tests/integration/reports',
                       help='æŠ¥å‘Šè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆè¿è¡ŒID
    run_id = f"golden_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # è¯»å–é…ç½®
    config = {
        "prompt_version": "current",
        "llm_model": "current",
        "test_suite": "golden_50"
    }
    if args.config:
        import json
        config.update(json.loads(args.config))
    
    print("=" * 80)
    print("FAA é»„é‡‘æµ‹è¯•é›†")
    print("=" * 80)
    print(f"è¿è¡ŒID: {run_id}")
    print(f"é…ç½®: {config}")
    print("=" * 80)
    
    # åˆå§‹åŒ–è¿è¡Œå™¨
    test_cases_file = Path(__file__).parent / "test_cases" / "golden_set.yaml"
    runner = GoldenSetRunner(test_cases_file)
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    runner.load_test_cases()
    
    # è®¾ç½®
    if not await runner.setup():
        print("âŒ åˆå§‹åŒ–å¤±è´¥")
        return 1
    
    try:
        # è¿è¡Œæµ‹è¯•
        await runner.run_all_tests(limit=args.limit)
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 80)
        print("æµ‹è¯•å®Œæˆï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
        print("=" * 80)
        
        summary_dict = runner.print_summary()
        
        # ç”ŸæˆæŠ¥å‘Š
        output_dir = Path(args.output_dir)
        report_file = SingleRunReporter.generate_report(
            run_id=run_id,
            config=config,
            summary=runner.test_scores[0].__class__.__module__.split('.')[0] if runner.test_scores else None,
            test_scores=runner.test_scores,
            output_dir=output_dir
        )
        
        # éœ€è¦é‡æ–°ç”Ÿæˆæ­£ç¡®çš„summaryå¯¹è±¡
        from validators.scoring import ScoringSystem
        summary = ScoringSystem.calculate_suite_summary(runner.test_scores)
        
        report_file = SingleRunReporter.generate_report(
            run_id=run_id,
            config=config,
            summary=summary,
            test_scores=runner.test_scores,
            output_dir=output_dir
        )
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   JSON: {report_file}")
        print(f"   TXT:  {report_file.with_suffix('.txt')}")
        
        # è¿”å›ç 
        if summary.pass_rate >= 0.9 and summary.avg_total_score >= 80:
            print("\nğŸ‰ é»„é‡‘æµ‹è¯•é›†é€šè¿‡ï¼")
            return 0
        else:
            print("\nâš ï¸  é»„é‡‘æµ‹è¯•é›†æœªè¾¾æ ‡")
            print(f"   é€šè¿‡ç‡: {summary.pass_rate*100:.1f}% (éœ€è¦>=90%)")
            print(f"   å¹³å‡åˆ†: {summary.avg_total_score:.1f} (éœ€è¦>=80)")
            return 1
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        await runner.teardown()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

