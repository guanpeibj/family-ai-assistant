#!/usr/bin/env python3
"""
è¿è¡ŒABæµ‹è¯•å¯¹æ¯”

å¯¹æ¯”ä¸¤ä¸ªä¸åŒçš„é…ç½®ï¼ˆPromptç‰ˆæœ¬ã€æ¨¡å‹ç­‰ï¼‰
"""

import asyncio
import sys
import argparse
from pathlib import Path
from datetime import datetime
import yaml
import json
import os

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from base_new import IntegrationTestBase
from reporters import ABComparisonReporter
from validators.scoring import ScoringSystem


class ABTestRunner:
    """ABæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, test_cases_file: Path):
        self.test_cases_file = test_cases_file
        self.test_cases = []
        
    def load_test_cases(self):
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        with open(self.test_cases_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        self.test_cases = []
        for category_name, cases in data.items():
            if isinstance(cases, list) and category_name not in ['test_suite_name', 'test_suite_version', 'total_cases']:
                self.test_cases.extend(cases)
        
        return len(self.test_cases)
    
    async def run_variant(
        self,
        variant_name: str,
        config: dict,
        limit: int = None
    ) -> IntegrationTestBase:
        """
        è¿è¡Œä¸€ä¸ªå˜ä½“
        
        Args:
            variant_name: å˜ä½“åç§°
            config: é…ç½®ï¼ˆå¯èƒ½åŒ…å«promptã€modelç­‰ï¼‰
            limit: é™åˆ¶æµ‹è¯•ç”¨ä¾‹æ•°é‡
            
        Returns:
            æµ‹è¯•è¿è¡Œå™¨å®ä¾‹ï¼ˆåŒ…å«æ‰€æœ‰è¯„åˆ†ï¼‰
        """
        print(f"\n{'='*80}")
        print(f"è¿è¡Œå˜ä½“: {variant_name}")
        print(f"é…ç½®: {config}")
        print(f"{'='*80}\n")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰
        original_env = {}
        if 'prompt_version' in config:
            # å¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡æ¥åˆ‡æ¢promptç‰ˆæœ¬
            pass
        if 'model' in config:
            original_env['OPENAI_MODEL'] = os.environ.get('OPENAI_MODEL')
            os.environ['OPENAI_MODEL'] = config['model']
        
        # åˆå§‹åŒ–è¿è¡Œå™¨
        runner = IntegrationTestBase(test_suite_name=f"ab_{variant_name}")
        
        # è®¾ç½®
        if not await runner.setup():
            raise Exception(f"{variant_name} åˆå§‹åŒ–å¤±è´¥")
        
        try:
            # è¿è¡Œæµ‹è¯•
            cases_to_run = self.test_cases[:limit] if limit else self.test_cases
            
            for i, test_case in enumerate(cases_to_run, 1):
                print(f"[{variant_name}] [{i}/{len(cases_to_run)}] {test_case['test_id']}...")
                
                await runner.run_test(
                    test_id=test_case['test_id'],
                    test_name=test_case['test_name'],
                    message=test_case['user_input'],
                    expected_behavior=test_case['expected_behavior'],
                    data_verification=test_case.get('data_verification')
                )
                
                await asyncio.sleep(0.3)
            
            return runner
            
        finally:
            # æ¢å¤ç¯å¢ƒå˜é‡
            for key, value in original_env.items():
                if value is not None:
                    os.environ[key] = value
                elif key in os.environ:
                    del os.environ[key]


async def main():
    parser = argparse.ArgumentParser(description='è¿è¡ŒFAA ABæµ‹è¯•')
    parser.add_argument('--variant-a', type=str, required=True,
                       help='å˜ä½“Aé…ç½®ï¼ˆJSONæ ¼å¼ï¼Œå¦‚ \'{"name":"v4_default","prompt":"v4_default"}\'ï¼‰')
    parser.add_argument('--variant-b', type=str, required=True,
                       help='å˜ä½“Bé…ç½®ï¼ˆJSONæ ¼å¼ï¼Œå¦‚ \'{"name":"v4_optimized","prompt":"v4_optimized"}\'ï¼‰')
    parser.add_argument('--limit', type=int, default=20,
                       help='é™åˆ¶è¿è¡Œçš„ç”¨ä¾‹æ•°é‡ï¼ˆé»˜è®¤20ï¼Œç”¨äºå¿«é€Ÿå¯¹æ¯”ï¼‰')
    parser.add_argument('--test-suite', type=str, default='golden_set',
                       help='æµ‹è¯•å¥—ä»¶ï¼ˆé»˜è®¤golden_setï¼‰')
    parser.add_argument('--output-dir', type=str, default='tests/integration/reports',
                       help='æŠ¥å‘Šè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # è§£æé…ç½®
    config_a = json.loads(args.variant_a)
    config_b = json.loads(args.variant_b)
    
    # ç”Ÿæˆå¯¹æ¯”ID
    comparison_id = f"{config_a.get('name', 'A')}_vs_{config_b.get('name', 'B')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    print("=" * 80)
    print("FAA ABæµ‹è¯•å¯¹æ¯”")
    print("=" * 80)
    print(f"å¯¹æ¯”ID: {comparison_id}")
    print(f"æµ‹è¯•å¥—ä»¶: {args.test_suite}")
    print(f"ç”¨ä¾‹æ•°é‡: {args.limit}")
    print("=" * 80)
    
    # åˆå§‹åŒ–ABæµ‹è¯•è¿è¡Œå™¨
    test_cases_file = Path(__file__).parent / "test_cases" / f"{args.test_suite}.yaml"
    ab_runner = ABTestRunner(test_cases_file)
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    total_cases = ab_runner.load_test_cases()
    print(f"âœ… åŠ è½½äº† {total_cases} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    try:
        # è¿è¡Œå˜ä½“A
        print(f"\nğŸ…°ï¸  å¼€å§‹è¿è¡Œå˜ä½“A...")
        runner_a = await ab_runner.run_variant("A", config_a, limit=args.limit)
        
        # è¿è¡Œå˜ä½“B
        print(f"\nğŸ…±ï¸  å¼€å§‹è¿è¡Œå˜ä½“B...")
        runner_b = await ab_runner.run_variant("B", config_b, limit=args.limit)
        
        # è®¡ç®—æ€»ç»“
        summary_a = ScoringSystem.calculate_suite_summary(runner_a.test_scores)
        summary_b = ScoringSystem.calculate_suite_summary(runner_b.test_scores)
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print("\n" + "=" * 80)
        print("å¯¹æ¯”ç»“æœ")
        print("=" * 80)
        
        print(f"\nğŸ…°ï¸  å˜ä½“A ({config_a.get('name', 'A')}):")
        print(f"   å¹³å‡åˆ†: {summary_a.avg_total_score:.1f}/100")
        print(f"   é€šè¿‡ç‡: {summary_a.pass_rate*100:.1f}%")
        print(f"   å¹³å‡è€—æ—¶: {summary_a.avg_duration:.1f}ç§’")
        
        print(f"\nğŸ…±ï¸  å˜ä½“B ({config_b.get('name', 'B')}):")
        print(f"   å¹³å‡åˆ†: {summary_b.avg_total_score:.1f}/100")
        print(f"   é€šè¿‡ç‡: {summary_b.pass_rate*100:.1f}%")
        print(f"   å¹³å‡è€—æ—¶: {summary_b.avg_duration:.1f}ç§’")
        
        print(f"\nğŸ“Š å·®å¼‚:")
        print(f"   åˆ†æ•°: {summary_b.avg_total_score - summary_a.avg_total_score:+.1f}")
        print(f"   è€—æ—¶: {summary_b.avg_duration - summary_a.avg_duration:+.1f}ç§’")
        print(f"   é€šè¿‡ç‡: {(summary_b.pass_rate - summary_a.pass_rate)*100:+.1f}%")
        
        # ç”ŸæˆæŠ¥å‘Š
        output_dir = Path(args.output_dir)
        report_file = ABComparisonReporter.generate_report(
            comparison_id=comparison_id,
            description=f"å¯¹æ¯” {config_a.get('name')} vs {config_b.get('name')}",
            variant_a_config=config_a,
            variant_b_config=config_b,
            summary_a=summary_a,
            summary_b=summary_b,
            output_dir=output_dir
        )
        
        print(f"\nğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   JSON: {report_file}")
        print(f"   TXT:  {report_file.with_suffix('.txt')}")
        
        # è¯»å–æ¨è
        with open(report_file, 'r', encoding='utf-8') as f:
            report_data = json.load(f)
        
        rec = report_data['recommendation']
        print(f"\nğŸ’¡ æ¨è:")
        winner_map = {"variant_a": f"å˜ä½“A ({config_a.get('name')})", 
                     "variant_b": f"å˜ä½“B ({config_b.get('name')})", 
                     "tie": "ä¸¤è€…ç›¸å½“"}
        print(f"   é€‰æ‹©: {winner_map[rec['overall']]}")
        print(f"   ç†ç”±: {rec['reason']}")
        print(f"   ç½®ä¿¡åº¦: {rec['confidence']*100:.0f}%")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ ABæµ‹è¯•å¼‚å¸¸ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

