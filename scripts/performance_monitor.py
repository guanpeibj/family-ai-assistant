#!/usr/bin/env python3
"""
FAA é€šç”¨åŒ–æ€§èƒ½ç›‘æ§ä¸åŠ¨æ€ä¼˜åŒ–ç³»ç»Ÿ

åŸºäºæ ¸å¿ƒç†å¿µï¼š
- æ•°æ®ç»“æ„æ³›åŒ–ï¼ŒæŸ¥è¯¢æ€§èƒ½ä¸“ä¸šåŒ–
- å·¥ç¨‹å›ºå®šï¼Œèƒ½åŠ›è‡ªåŠ¨å¢é•¿
- AIé©±åŠ¨å†³ç­–ï¼Œç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–
"""
import asyncio
import asyncpg
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict
import structlog

logger = structlog.get_logger()

class FAAPerfomanceMonitor:
    """FAA æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–ç³»ç»Ÿ"""
    
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        self.pool = await asyncpg.create_pool(self.database_url)
        
    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.pool:
            await self.pool.close()
    
    async def analyze_query_patterns(self) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢æ¨¡å¼ï¼Œè¯†åˆ«é«˜é¢‘æŸ¥è¯¢ç»„åˆ
        """
        try:
            async with self.pool.acquire() as conn:
                # åˆ†ææœ€è¿‘30å¤©çš„æŸ¥è¯¢æ¨¡å¼
                analysis = await conn.fetch("""
                    SELECT 
                        type_extracted,
                        person_extracted,
                        metric_extracted,
                        subject_extracted,
                        category_extracted,
                        COUNT(*) as query_count,
                        COUNT(DISTINCT user_id) as user_count,
                        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY value_extracted) as median_value,
                        AVG(value_extracted) as avg_value
                    FROM memories 
                    WHERE created_at >= NOW() - INTERVAL '30 days'
                      AND (type_extracted IS NOT NULL OR person_extracted IS NOT NULL)
                    GROUP BY type_extracted, person_extracted, metric_extracted, subject_extracted, category_extracted
                    HAVING COUNT(*) > 10
                    ORDER BY query_count DESC
                    LIMIT 50
                """)
                
                patterns = {}
                for row in analysis:
                    pattern_key = self._generate_pattern_key(dict(row))
                    patterns[pattern_key] = {
                        "query_count": row['query_count'],
                        "user_count": row['user_count'],
                        "data_distribution": {
                            "median_value": float(row['median_value']) if row['median_value'] else None,
                            "avg_value": float(row['avg_value']) if row['avg_value'] else None
                        },
                        "fields": {
                            "type": row['type_extracted'],
                            "person": row['person_extracted'],
                            "metric": row['metric_extracted'],
                            "subject": row['subject_extracted'],
                            "category": row['category_extracted']
                        }
                    }
                
                return {
                    "analysis_date": datetime.now().isoformat(),
                    "total_patterns": len(patterns),
                    "patterns": patterns
                }
                
        except Exception as e:
            logger.error("æŸ¥è¯¢æ¨¡å¼åˆ†æå¤±è´¥", error=str(e))
            return {"error": str(e)}
    
    async def suggest_optimizations(self) -> Dict[str, Any]:
        """
        åŸºäºæŸ¥è¯¢æ¨¡å¼å»ºè®®ä¼˜åŒ–ç­–ç•¥
        """
        try:
            patterns = await self.analyze_query_patterns()
            if "error" in patterns:
                return patterns
            
            suggestions = []
            
            # åˆ†æé«˜é¢‘æŸ¥è¯¢ç»„åˆ
            for pattern_key, pattern_data in patterns["patterns"].items():
                query_count = pattern_data["query_count"]
                fields = pattern_data["fields"]
                
                # è¯†åˆ«éœ€è¦ä¸“ç”¨ç´¢å¼•çš„æŸ¥è¯¢æ¨¡å¼
                if query_count > 100:  # é«˜é¢‘æŸ¥è¯¢
                    index_suggestion = self._generate_index_suggestion(fields, query_count)
                    if index_suggestion:
                        suggestions.append(index_suggestion)
                
                # è¯†åˆ«éœ€è¦ä¸“ç”¨æŸ¥è¯¢å‡½æ•°çš„æ¨¡å¼
                if query_count > 200 and pattern_data["user_count"] > 3:
                    function_suggestion = self._generate_function_suggestion(fields, pattern_data)
                    if function_suggestion:
                        suggestions.append(function_suggestion)
            
            # åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
            index_analysis = await self._analyze_index_usage()
            suggestions.extend(index_analysis.get("suggestions", []))
            
            return {
                "analysis_date": datetime.now().isoformat(),
                "total_suggestions": len(suggestions),
                "suggestions": suggestions,
                "patterns_analyzed": len(patterns["patterns"]),
                "index_analysis": index_analysis
            }
            
        except Exception as e:
            logger.error("ä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥", error=str(e))
            return {"error": str(e)}
    
    async def _analyze_index_usage(self) -> Dict[str, Any]:
        """åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        try:
            async with self.pool.acquire() as conn:
                # è·å–ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡
                index_stats = await conn.fetch("""
                    SELECT * FROM index_usage_analysis
                    WHERE scan_count IS NOT NULL
                    ORDER BY scan_count DESC
                """)
                
                suggestions = []
                unused_indexes = []
                high_usage_indexes = []
                
                for row in index_stats:
                    usage_level = row['usage_level']
                    index_name = row['indexname']
                    scan_count = row['scan_count']
                    
                    if usage_level == 'UNUSED':
                        unused_indexes.append({
                            "index_name": index_name,
                            "suggestion": f"è€ƒè™‘åˆ é™¤æœªä½¿ç”¨çš„ç´¢å¼•: {index_name}",
                            "reason": "ç´¢å¼•ä»æœªè¢«ä½¿ç”¨ï¼Œæµªè´¹å­˜å‚¨ç©ºé—´å’Œç»´æŠ¤æˆæœ¬"
                        })
                    elif usage_level == 'HIGH_USAGE':
                        high_usage_indexes.append({
                            "index_name": index_name,
                            "scan_count": scan_count,
                            "suggestion": f"é«˜ä½¿ç”¨ç‡ç´¢å¼• {index_name}ï¼Œè€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–",
                            "reason": "é«˜é¢‘ä½¿ç”¨çš„ç´¢å¼•ï¼Œå€¼å¾—æŠ•å…¥æ›´å¤šä¼˜åŒ–èµ„æº"
                        })
                
                # ç”Ÿæˆæ¸…ç†å»ºè®®
                if len(unused_indexes) > 2:
                    suggestions.append({
                        "type": "index_cleanup",
                        "priority": "medium",
                        "action": f"åˆ é™¤ {len(unused_indexes)} ä¸ªæœªä½¿ç”¨ç´¢å¼•",
                        "details": unused_indexes
                    })
                
                # ç”Ÿæˆé‡ç‚¹ä¼˜åŒ–å»ºè®®
                if high_usage_indexes:
                    suggestions.append({
                        "type": "index_optimization", 
                        "priority": "high",
                        "action": f"ä¼˜åŒ– {len(high_usage_indexes)} ä¸ªé«˜ä½¿ç”¨ç‡ç´¢å¼•",
                        "details": high_usage_indexes
                    })
                
                return {
                    "unused_count": len(unused_indexes),
                    "high_usage_count": len(high_usage_indexes),
                    "suggestions": suggestions
                }
                
        except Exception as e:
            logger.error("ç´¢å¼•åˆ†æå¤±è´¥", error=str(e))
            return {"error": str(e), "suggestions": []}
    
    def _generate_pattern_key(self, row_data: Dict) -> str:
        """ç”ŸæˆæŸ¥è¯¢æ¨¡å¼çš„å”¯ä¸€æ ‡è¯†"""
        key_parts = []
        for field in ['type_extracted', 'person_extracted', 'metric_extracted', 'subject_extracted', 'category_extracted']:
            value = row_data.get(field)
            if value:
                key_parts.append(f"{field.replace('_extracted', '')}:{value}")
        return "|".join(key_parts) if key_parts else "general"
    
    def _generate_index_suggestion(self, fields: Dict, query_count: int) -> Optional[Dict]:
        """åŸºäºå­—æ®µç»„åˆç”Ÿæˆç´¢å¼•å»ºè®®"""
        non_null_fields = {k: v for k, v in fields.items() if v is not None}
        
        if len(non_null_fields) < 2:
            return None
        
        # ç”Ÿæˆç´¢å¼•åç§°
        field_names = list(non_null_fields.keys())
        index_name = f"idx_auto_{'_'.join(field_names)}_{hash(str(sorted(non_null_fields.items()))) % 10000}"
        
        # ç”Ÿæˆç´¢å¼•åˆ›å»ºè¯­å¥
        index_columns = []
        where_conditions = []
        
        for field, value in non_null_fields.items():
            if field == 'type':
                index_columns.append("user_id")
                index_columns.append("type_extracted") 
                where_conditions.append(f"type_extracted = '{value}'")
            elif field in ['person', 'metric', 'subject', 'category']:
                index_columns.append(f"{field}_extracted")
        
        # æ·»åŠ å¸¸ç”¨çš„æ’åºå’ŒæŸ¥è¯¢åˆ—
        index_columns.extend(["occurred_at DESC", "value_extracted"])
        
        create_statement = f"""
        CREATE INDEX CONCURRENTLY {index_name}
        ON memories ({', '.join(index_columns)})"""
        
        if where_conditions:
            create_statement += f"\nWHERE {' AND '.join(where_conditions)}"
        
        return {
            "type": "dynamic_index",
            "priority": "high" if query_count > 500 else "medium",
            "index_name": index_name,
            "create_statement": create_statement.strip() + ";",
            "reason": f"é«˜é¢‘æŸ¥è¯¢ç»„åˆ (ä½¿ç”¨ {query_count} æ¬¡)ï¼Œéœ€è¦ä¸“ç”¨ç´¢å¼•ä¼˜åŒ–",
            "estimated_benefit": f"é¢„è®¡æå‡æŸ¥è¯¢é€Ÿåº¦ 10-50å€",
            "query_count": query_count
        }
    
    def _generate_function_suggestion(self, fields: Dict, pattern_data: Dict) -> Optional[Dict]:
        """åŸºäºæŸ¥è¯¢æ¨¡å¼ç”Ÿæˆä¸“ç”¨å‡½æ•°å»ºè®®"""
        non_null_fields = {k: v for k, v in fields.items() if v is not None}
        
        if 'type' not in non_null_fields:
            return None
        
        data_type = non_null_fields['type']
        query_count = pattern_data["query_count"]
        
        # ç”Ÿæˆå‡½æ•°åç§°
        function_name = f"get_{data_type}_summary_by"
        if 'person' in non_null_fields:
            function_name += "_person"
        if 'metric' in non_null_fields:
            function_name += "_metric"
        if 'subject' in non_null_fields:
            function_name += "_subject"
        
        return {
            "type": "specialized_function",
            "priority": "high",
            "function_name": function_name,
            "data_type": data_type,
            "reason": f"é«˜é¢‘ä¸“ç”¨æŸ¥è¯¢ (ä½¿ç”¨ {query_count} æ¬¡)ï¼Œå»ºè®®åˆ›å»ºä¸“ç”¨ä¼˜åŒ–å‡½æ•°",
            "estimated_benefit": "æ•°æ®åº“ç«¯èšåˆï¼Œå‡å°‘æ•°æ®ä¼ è¾“ï¼Œæå‡5-10å€æ€§èƒ½",
            "query_count": query_count,
            "user_count": pattern_data["user_count"]
        }
    
    async def apply_optimization(self, suggestion: Dict[str, Any]) -> Dict[str, Any]:
        """
        åº”ç”¨ä¼˜åŒ–å»ºè®®
        """
        try:
            async with self.pool.acquire() as conn:
                suggestion_type = suggestion.get("type")
                
                if suggestion_type == "dynamic_index":
                    # åˆ›å»ºåŠ¨æ€ç´¢å¼•
                    create_statement = suggestion["create_statement"]
                    await conn.execute(create_statement)
                    
                    return {
                        "success": True,
                        "action": "index_created",
                        "index_name": suggestion["index_name"],
                        "message": f"æˆåŠŸåˆ›å»ºç´¢å¼•: {suggestion['index_name']}"
                    }
                
                elif suggestion_type == "index_cleanup":
                    # æ¸…ç†æœªä½¿ç”¨çš„ç´¢å¼•
                    cleaned_count = 0
                    for index_info in suggestion.get("details", []):
                        index_name = index_info["index_name"]
                        if not index_name.startswith("idx_memories_financial"):  # ä¿æŠ¤æ ¸å¿ƒç´¢å¼•
                            try:
                                await conn.execute(f"DROP INDEX IF EXISTS {index_name}")
                                cleaned_count += 1
                            except Exception as e:
                                logger.warning("ç´¢å¼•åˆ é™¤å¤±è´¥", index=index_name, error=str(e))
                    
                    return {
                        "success": True,
                        "action": "indexes_cleaned",
                        "cleaned_count": cleaned_count,
                        "message": f"æˆåŠŸæ¸…ç† {cleaned_count} ä¸ªæœªä½¿ç”¨ç´¢å¼•"
                    }
                
                else:
                    return {
                        "success": False,
                        "error": f"ä¸æ”¯æŒçš„ä¼˜åŒ–ç±»å‹: {suggestion_type}"
                    }
                    
        except Exception as e:
            logger.error("ä¼˜åŒ–åº”ç”¨å¤±è´¥", suggestion=suggestion, error=str(e))
            return {
                "success": False,
                "error": str(e)
            }
    
    async def generate_performance_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆå®Œæ•´çš„æ€§èƒ½æŠ¥å‘Š
        """
        try:
            # æ”¶é›†å„ç§æ€§èƒ½æŒ‡æ ‡
            query_patterns = await self.analyze_query_patterns()
            optimization_suggestions = await self.suggest_optimizations()
            
            # è®¡ç®—å…³é”®æ€§èƒ½æŒ‡æ ‡
            async with self.pool.acquire() as conn:
                # æ•°æ®ç»Ÿè®¡
                data_stats = await conn.fetchrow("""
                    SELECT 
                        COUNT(*) as total_records,
                        COUNT(DISTINCT user_id) as total_users,
                        COUNT(DISTINCT type_extracted) as data_types,
                        COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '7 days') as recent_records,
                        AVG(pg_column_size(ai_understanding)) as avg_jsonb_size
                    FROM memories
                """)
                
                # è®¡ç®—åˆ—ä¼˜åŒ–æ•ˆæœ
                computed_col_usage = await conn.fetch("""
                    SELECT 
                        'type_extracted' as column_name,
                        COUNT(*) FILTER (WHERE type_extracted IS NOT NULL) as non_null_count,
                        COUNT(DISTINCT type_extracted) as distinct_values
                    FROM memories
                    UNION ALL
                    SELECT 
                        'person_extracted',
                        COUNT(*) FILTER (WHERE person_extracted IS NOT NULL),
                        COUNT(DISTINCT person_extracted)
                    FROM memories
                    UNION ALL
                    SELECT 
                        'metric_extracted',
                        COUNT(*) FILTER (WHERE metric_extracted IS NOT NULL),
                        COUNT(DISTINCT metric_extracted)
                    FROM memories
                """)
            
            computed_col_stats = {}
            for row in computed_col_usage:
                computed_col_stats[row['column_name']] = {
                    "non_null_count": row['non_null_count'],
                    "distinct_values": row['distinct_values'],
                    "utilization_rate": row['non_null_count'] / data_stats['total_records'] if data_stats['total_records'] > 0 else 0
                }
            
            return {
                "report_date": datetime.now().isoformat(),
                "summary": {
                    "total_records": data_stats['total_records'],
                    "total_users": data_stats['total_users'],
                    "data_types": data_stats['data_types'],
                    "recent_activity": data_stats['recent_records'],
                    "avg_jsonb_size_bytes": int(data_stats['avg_jsonb_size']) if data_stats['avg_jsonb_size'] else 0
                },
                "computed_columns_performance": computed_col_stats,
                "query_patterns": query_patterns,
                "optimization_suggestions": optimization_suggestions,
                "performance_score": self._calculate_performance_score(data_stats, computed_col_stats, optimization_suggestions)
            }
            
        except Exception as e:
            logger.error("æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå¤±è´¥", error=str(e))
            return {"error": str(e)}
    
    def _calculate_performance_score(self, data_stats: Dict, computed_col_stats: Dict, optimization_suggestions: Dict) -> Dict[str, Any]:
        """è®¡ç®—ç³»ç»Ÿæ€§èƒ½è¯„åˆ†"""
        score = 100  # åŸºç¡€åˆ†æ•°
        issues = []
        
        # æ•°æ®é‡è¯„ä¼°
        total_records = data_stats['total_records']
        if total_records > 100000:
            score -= 10
            issues.append("æ•°æ®é‡è¾ƒå¤§ï¼Œéœ€è¦æŒç»­ä¼˜åŒ–")
        
        # è®¡ç®—åˆ—åˆ©ç”¨ç‡è¯„ä¼°
        for col_name, stats in computed_col_stats.items():
            utilization = stats['utilization_rate']
            if utilization < 0.1:  # åˆ©ç”¨ç‡ä½äº10%
                score -= 5
                issues.append(f"{col_name} åˆ©ç”¨ç‡è¿‡ä½ ({utilization:.1%})")
        
        # ä¼˜åŒ–å»ºè®®è¯„ä¼°
        high_priority_suggestions = len([s for s in optimization_suggestions.get("suggestions", []) if s.get("priority") == "high"])
        score -= high_priority_suggestions * 3
        
        if high_priority_suggestions > 0:
            issues.append(f"{high_priority_suggestions} ä¸ªé«˜ä¼˜å…ˆçº§ä¼˜åŒ–å»ºè®®")
        
        return {
            "score": max(score, 0),
            "grade": self._get_performance_grade(score),
            "issues_count": len(issues),
            "issues": issues
        }
    
    def _get_performance_grade(self, score: int) -> str:
        """æ ¹æ®åˆ†æ•°è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 90:
            return "A (ä¼˜ç§€)"
        elif score >= 80:
            return "B (è‰¯å¥½)"
        elif score >= 70:
            return "C (ä¸€èˆ¬)"
        elif score >= 60:
            return "D (éœ€æ”¹è¿›)"
        else:
            return "F (éœ€ç«‹å³ä¼˜åŒ–)"

async def main():
    """ä¸»å‡½æ•° - æ€§èƒ½ç›‘æ§ç¤ºä¾‹"""
    database_url = os.getenv('DATABASE_URL', 'postgresql://faa:faa@localhost:5432/family_assistant')
    
    monitor = FAAPerfomanceMonitor(database_url)
    await monitor.initialize()
    
    try:
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = await monitor.generate_performance_report()
        print("=== FAA æ€§èƒ½ç›‘æ§æŠ¥å‘Š ===")
        print(json.dumps(report, indent=2, ensure_ascii=False))
        
        # åˆ†æä¼˜åŒ–å»ºè®®
        suggestions = await monitor.suggest_optimizations()
        print("\n=== ä¼˜åŒ–å»ºè®® ===")
        for suggestion in suggestions.get("suggestions", []):
            print(f"- {suggestion.get('reason', 'N/A')}")
            if suggestion.get("priority") == "high":
                print(f"  ä¼˜å…ˆçº§: ğŸ”´ {suggestion['priority']}")
            else:
                print(f"  ä¼˜å…ˆçº§: ğŸŸ¡ {suggestion['priority']}")
        
    finally:
        await monitor.close()

if __name__ == "__main__":
    asyncio.run(main())
