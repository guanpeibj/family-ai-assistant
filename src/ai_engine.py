"""
AIé©±åŠ¨çš„æ ¸å¿ƒå¼•æ“ - è®©AIå†³å®šä¸€åˆ‡
"""
import json
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
import uuid
import time
from pydantic import BaseModel, Field, ValidationError
import openai
import structlog
import httpx
import os

from .core.config import settings
from .core.prompt_manager import prompt_manager
from .core.llm_client import LLMClient
from .core.config import settings
from .services.media_service import make_signed_url

logger = structlog.get_logger(__name__)

def _looks_like_uuid(value: Optional[str]) -> bool:
    if not value or not isinstance(value, str):
        return False
    try:
        uuid.UUID(value)
        return True
    except Exception:
        return False

# å®¶åº­AIåŠ©æ‰‹çš„ç³»ç»Ÿæç¤ºè¯
FAMILY_AI_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªè´´å¿ƒçš„å®¶åº­AIåŠ©æ‰‹ï¼Œä¸“é—¨æœåŠ¡äºä¸€ä¸ªæœ‰3ä¸ªå­©å­çš„å®¶åº­ã€‚

ä½ çš„æ ¸å¿ƒèƒ½åŠ›ï¼š
1. è®°è´¦ç®¡ç†ï¼šè¯†åˆ«å¹¶è®°å½•å®¶åº­æ”¶æ”¯ï¼Œæä¾›ç»Ÿè®¡åˆ†æå’Œé¢„ç®—å»ºè®®
2. å¥åº·è¿½è¸ªï¼šè®°å½•å®¶äººå¥åº·æ•°æ®ï¼ˆèº«é«˜ã€ä½“é‡ã€ç–«è‹—ç­‰ï¼‰ï¼Œè·Ÿè¸ªå˜åŒ–è¶‹åŠ¿
3. æ‚äº‹æé†’ï¼šç®¡ç†æ—¥å¸¸äº‹åŠ¡ï¼ŒåŠæ—¶æé†’é‡è¦äº‹é¡¹

å›å¤åŸåˆ™ï¼š
- æ¸©é¦¨å‹å¥½ï¼Œåƒå®¶äººèˆ¬å…³æ€€
- ç®€æ´å®ç”¨ï¼Œä¸è¯´åºŸè¯
- ä¸»åŠ¨æä¾›æœ‰ä»·å€¼çš„ç»Ÿè®¡å’Œå»ºè®®
- è®°ä½è¿™æ˜¯ä¸€ä¸ªæœ‰3ä¸ªå­©å­çš„å®¶åº­ï¼Œå…³æ³¨è‚²å„¿ç›¸å…³éœ€æ±‚

ä¿¡æ¯ç†è§£æŒ‡å—ï¼š
- "ä»Šå¤©/æ˜¨å¤©/ä¸Šå‘¨"ç­‰æ—¶é—´è¡¨è¾¾è¦è½¬æ¢ä¸ºå…·ä½“æ—¥æœŸ
- è¯†åˆ«å®¶åº­æˆå‘˜ï¼šå„¿å­ã€å¥³å„¿ï¼ˆå¤§å¥³å„¿ã€äºŒå¥³å„¿ï¼‰ã€å¦»å­ã€æˆ‘/è€å…¬
- æ”¯å‡ºè‡ªåŠ¨åˆ†ç±»ï¼šé¤é¥®ã€è´­ç‰©ã€äº¤é€šã€åŒ»ç–—ã€æ•™è‚²ã€æ—¥ç”¨å“ç­‰
- å¦‚æœæåˆ°"æ›´æ–°"æˆ–"æ”¹ä¸º"ï¼Œè¦è¦†ç›–ä¹‹å‰çš„è®°å½•

ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ä»¥ä½¿ç”¨ï¼š
- store: å­˜å‚¨ä»»ä½•é‡è¦ä¿¡æ¯ï¼ˆæ”¯å‡ºã€æ”¶å…¥ã€å¥åº·æ•°æ®ã€æ‚äº‹ç­‰ï¼‰
- search: æŸ¥æ‰¾å†å²è®°å½•
- aggregate: ç»Ÿè®¡åˆ†ææ•°æ®ï¼ˆæ±‚å’Œã€è®¡æ•°ã€å¹³å‡å€¼ç­‰ï¼‰
- schedule_reminder: è®¾ç½®æé†’
- get_pending_reminders: æŸ¥çœ‹å¾…å‘é€æé†’
- mark_reminder_sent: æ ‡è®°æé†’å·²å‘é€
"""


class AIEngine:
    def __init__(self):
        # ç»Ÿä¸€ LLM å®¢æˆ·ç«¯ï¼ˆå¯æŒ‰é…ç½®åˆ‡æ¢ OpenAI å…¼å®¹/Anthropic ç­‰ï¼‰
        self.llm = LLMClient()
        self.mcp_client = None
        self.mcp_url = os.getenv('MCP_SERVER_URL', 'http://faa-mcp:8000')
        
    async def initialize_mcp(self):
        """åˆå§‹åŒ–MCPå®¢æˆ·ç«¯è¿æ¥"""
        try:
            # æš‚æ—¶ä½¿ç”¨HTTPè°ƒç”¨æ–¹å¼ï¼Œè€Œä¸æ˜¯stdio
            # è¿™æ ·æ›´ç®€å•ä¸”é€‚åˆå®¹å™¨åŒ–éƒ¨ç½²
            logger.info(f"Connecting to MCP server at {self.mcp_url}")
            
            # æµ‹è¯•è¿æ¥
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.mcp_url}/health", timeout=5.0)
                if response.status_code == 200:
                    logger.info("MCP server is healthy")
                    self.mcp_client = True  # æ ‡è®°ä¸ºå¯ç”¨
                else:
                    logger.warning(f"MCP server health check failed: {response.status_code}")
            
        except Exception as e:
            logger.error(f"Failed to connect to MCP server: {e}")
            logger.warning("Falling back to mock MCP mode")
            self.mcp_client = None
    
    async def close(self):
        """å…³é—­MCPå®¢æˆ·ç«¯è¿æ¥"""
        # HTTPå®¢æˆ·ç«¯æ— éœ€ç‰¹æ®Šå…³é—­
        pass
    
    async def process_message(self, content: str, user_id: str, context: Dict[str, Any] = None) -> str:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯ - å®Œå…¨ç”±AIé©±åŠ¨
        
        Args:
            content: æ¶ˆæ¯å†…å®¹ï¼ˆå·²è§£å¯†çš„æ–‡æœ¬ï¼‰
            user_id: ç”¨æˆ·ID
            context: æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆchannelã€sender_idç­‰ï¼Œè®©AIç†è§£ï¼‰
        """
        # è´¯ç©¿å¼ trace_id
        trace_id = str(uuid.uuid4())
        thread_id = (context or {}).get('thread_id') if context else None
        channel = (context or {}).get('channel') if context else None

        logger.info(
            "message.received",
            trace_id=trace_id,
            user_id=user_id,
            thread_id=thread_id,
            channel=channel,
            content_preview=content[:200]
        )

        try:
            # å°†é™„ä»¶çš„è¡ç”Ÿæ–‡æœ¬çº³å…¥å¯æ£€ç´¢/åµŒå…¥çš„è¯­å¢ƒï¼ˆM1ï¼šå…ˆæ‹¼æ¥æ–‡æœ¬ï¼Œä¸åšå¤æ‚æƒé‡ï¼‰
            attachments = (context or {}).get('attachments') if context else None
            derived_texts: List[str] = []
            if isinstance(attachments, list):
                for att in attachments:
                    if not isinstance(att, dict):
                        continue
                    # é¢„ç•™å­—æ®µåï¼Œåç»­å¯ç”±é¢„å¤„ç†æ¨¡å—å¡«å……
                    tx = att.get('transcription', {}).get('text') if isinstance(att.get('transcription'), dict) else None
                    if not tx:
                        tx = att.get('ocr_text')
                    if not tx:
                        tx = att.get('vision_summary')
                    if tx:
                        derived_texts.append(str(tx))
            if derived_texts:
                content = (content or '').strip()
                extra = "\n\n[æ¥è‡ªé™„ä»¶çš„æ–‡æœ¬]\n" + "\n".join(derived_texts)
                content = (content + extra) if content else "\n".join(derived_texts)
            # ç¬¬ä¸€æ­¥ï¼šç†è§£ç”¨æˆ·æ„å›¾å’Œæå–ä¿¡æ¯
            understanding = await self._understand_message(content, user_id, context, trace_id=trace_id)
            
            # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œå¿…è¦çš„æ“ä½œ
            result = await self._execute_actions(understanding, user_id, context=context, trace_id=trace_id)
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå›å¤
            response = await self._generate_response(content, understanding, result, context)

            # å­˜å‚¨å¯¹è¯å›åˆï¼ˆç”¨æˆ·ä¸åŠ©æ‰‹å„ä¸€æ¡ï¼‰ï¼Œç”¨äºè¿ç»­å¯¹è¯ä¸åç»­æ£€ç´¢
            try:
                await self._store_chat_turns(
                    user_id=user_id,
                    thread_id=thread_id,
                    trace_id=trace_id,
                    user_message=content,
                    assistant_message=response,
                    understanding=understanding,
                    context=context,
                )
            except Exception as e:
                logger.error("store.chat_turns.failed", trace_id=trace_id, error=str(e))

            # è§¦å‘ä¼šè¯æ‘˜è¦ï¼ˆå¼‚æ­¥çŸ­è·¯ï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            try:
                await self._maybe_summarize_thread(user_id=user_id, thread_id=thread_id, trace_id=trace_id)
            except Exception as e:
                logger.warning("thread.summarize.skip", trace_id=trace_id, error=str(e))

            # è½ç›˜äº¤äº’è½¨è¿¹ï¼Œä¾¿äºæ’éšœ
            try:
                await self._persist_interaction(
                    trace_id=trace_id,
                    user_id=user_id,
                    thread_id=thread_id,
                    channel=channel,
                    message_id=(context or {}).get('message_id') if context else None,
                    input_text=content,
                    understanding=understanding,
                    actions=result,
                    response_text=response,
                )
            except Exception as e:
                logger.error("interaction.persist.failed", trace_id=trace_id, error=str(e))
            
            return response
            
        except Exception as e:
            logger.error("message.process.error", trace_id=trace_id, error=str(e))
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
    
    async def _get_recent_memories(self, user_id: str, limit: int = 5, thread_id: Optional[str] = None, *, shared_thread: bool = False, channel: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·æœ€è¿‘çš„äº¤äº’è®°å½•ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£

        Args:
            user_id: å½’å±ç”¨æˆ·
            limit: è¿”å›æ•°é‡ä¸Šé™
            thread_id: çº¿ç¨‹æ ‡è¯†ï¼ˆç”¨äºè¿ç»­ä¸Šä¸‹æ–‡ï¼‰
            shared_thread: æ˜¯å¦æŒ‰çº¿ç¨‹è·¨ç”¨æˆ·å…±äº«æ£€ç´¢
            channel: æ¸ é“ï¼ˆå¦‚ threema/apiï¼‰ï¼Œç”¨äºåœ¨å…±äº«çº¿ç¨‹ä¸‹è¿›ä¸€æ­¥é™å®š
        """
        try:
            # è·å–æœ€è¿‘çš„è®°å¿†
            filters = {'limit': limit}
            if thread_id:
                filters['thread_id'] = thread_id
                filters['type'] = 'chat_turn'
                if shared_thread:
                    filters['shared_thread'] = True
                if channel:
                    filters['channel'] = channel
            recent_memories = await self._call_mcp_tool(
                'search',
                query='',  # ç©ºæŸ¥è¯¢è·å–æœ€æ–°è®°å½•
                user_id=user_id,
                filters=filters
            )
            
            # æ ¼å¼åŒ–è®°å¿†ï¼Œæå–å…³é”®ä¿¡æ¯
            formatted_memories = []
            for memory in recent_memories:
                if isinstance(memory, dict):
                    formatted_memories.append({
                        'content': memory.get('content', ''),
                        'ai_understanding': memory.get('ai_understanding', {}),
                        'time': memory.get('occurred_at', '')
                    })
            
            return formatted_memories
            
        except Exception as e:
            logger.error(f"Error getting recent memories: {e}")
            return []
    
    async def _understand_message(self, content: str, user_id: str, context: Dict[str, Any] = None, *, trace_id: str) -> Dict[str, Any]:
        """
        AIç†è§£æ¶ˆæ¯å†…å®¹ - å¢å¼ºç‰ˆï¼ŒåŒ…å«å†å²ä¸Šä¸‹æ–‡å’Œä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥
        """
        # ä¸Šä¸‹æ–‡æ„å»ºï¼šè¿‘æœŸå¯¹è¯çª—å£ + è¯­ä¹‰æ£€ç´¢ + æ‘˜è¦ï¼ˆæ”¹ä¸ºåˆ†å—é…é¢ä¸é‡æ’å»é‡ï¼‰
        thread_id = (context or {}).get('thread_id') if context else None
        channel = (context or {}).get('channel') if context else None
        # å…±äº«çº¿ç¨‹ç­–ç•¥ï¼šå½“ä¸Šä¸‹æ–‡æŒ‡ç¤º shared_thread/conversation_scope=shared æ—¶å¯ç”¨
        shared_thread = False
        if context:
            if context.get('shared_thread') is True:
                shared_thread = True
            if context.get('conversation_scope') == 'shared':
                shared_thread = True
        recent_memories = await self._get_recent_memories(
            user_id,
            limit=10,
            thread_id=thread_id,
            shared_thread=shared_thread,
            channel=channel
        )
        semantic_related = await self._semantic_search(
            user_id,
            query=content,
            top_k=5,
            thread_id=thread_id,
            shared_thread=shared_thread,
            channel=channel
        )
        thread_summaries = await self._get_recent_thread_summaries(user_id, thread_id, limit=1, shared_thread=shared_thread, channel=channel)
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = ""
        if context:
            context_info = f"\næ¶ˆæ¯æ¥æºï¼š{context.get('channel', 'æœªçŸ¥')}"
            if context.get('sender_id'):
                context_info += f"\nå‘é€è€…IDï¼š{context['sender_id']}"
            if context.get('nickname'):
                context_info += f"\nå‘é€è€…æ˜µç§°ï¼š{context['nickname']}"
        
        # åˆ†å—é…é¢ + é‡æ’å»é‡
        def normalize_key(m: Dict[str, Any]) -> str:
            aiu = m.get('ai_understanding') or {}
            intent = aiu.get('intent') if isinstance(aiu, dict) else None
            when = m.get('time') or m.get('occurred_at') or ''
            return f"{m.get('content','')}||{intent or ''}||{when}"

        def fmt_with_budget(items: List[Dict[str, Any]], title: str, char_budget: int, start_index: int = 1, seen: Optional[set] = None) -> Tuple[str, int, int]:
            seen_local = seen if seen is not None else set()
            block_lines: List[str] = []
            count_included = 0
            for idx, m in enumerate(items, start_index):
                key = normalize_key(m)
                if key in seen_local:
                    continue
                seen_local.add(key)
                content_line = m.get('content', '')
                aiu = m.get('ai_understanding') if isinstance(m.get('ai_understanding'), dict) else {}
                intent = aiu.get('intent') if isinstance(aiu, dict) else None
                when = m.get('time') or m.get('occurred_at') or ''
                line = f"\n{idx}. {when}: {content_line}"
                if intent:
                    line += f" (æ„å›¾: {intent})"
                # é¢„ä¼°æ·»åŠ åé•¿åº¦
                projected = (0 if not block_lines else len(''.join(block_lines))) + len(line)
                if projected > char_budget and count_included > 0:
                    break
                block_lines.append(line)
                count_included += 1
            if not block_lines:
                return "", start_index, 0
            header = f"\n\n{title}:"
            text = header + ''.join(block_lines)
            return text, start_index + count_included, count_included

        # é¢„ç®—ï¼šæ€» 3500ï¼ŒæŒ‰ æ‘˜è¦:600 / æœ€è¿‘:2100 / è¯­ä¹‰:800 åˆ†é…
        budget_summary = 600
        budget_recent = 2100
        budget_semantic = 800
        seen_keys: set = set()
        running_index = 1
        history_parts: List[str] = []

        # ä¼˜å…ˆçº§ï¼šæ‘˜è¦ > æœ€è¿‘ > è¯­ä¹‰
        if thread_summaries:
            text, running_index, _ = fmt_with_budget(thread_summaries, "ä¼šè¯æ‘˜è¦", budget_summary, running_index, seen_keys)
            if text:
                history_parts.append(text)
        if recent_memories:
            text, running_index, _ = fmt_with_budget(recent_memories, "æœ€è¿‘çš„äº¤äº’å†å²ï¼ˆç”¨äºç†è§£ä¸Šä¸‹æ–‡ï¼‰", budget_recent, running_index, seen_keys)
            if text:
                history_parts.append(text)
        if semantic_related:
            text, running_index, _ = fmt_with_budget(semantic_related, "ä¸å½“å‰é—®é¢˜è¯­ä¹‰ç›¸å…³çš„å†å²", budget_semantic, running_index, seen_keys)
            if text:
                history_parts.append(text)

        history_context = ''.join(history_parts)
        # å…œåº•å†æˆªæ–­ï¼ˆæå°‘æ•°æƒ…å†µä¸‹ä¸‰å—ä¹‹å’Œä»å¯èƒ½ç•¥è¶…å‡ºï¼‰
        if len(history_context) > 3500:
            history_context = history_context[:3500]

        # æ‰“å°æœ€ç»ˆä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°æ—¥å¿—
        try:
            logger.info(
                "llm.context.built",
                trace_id=trace_id,
                thread_id=thread_id,
                channel=channel,
                shared_thread=shared_thread,
                context_length=len(history_context),
                context_preview=history_context[:500]
            )
        except Exception:
            pass
        
        # è·å–å½“å‰æ—¶é—´ç”¨äºæ—¶é—´ç†è§£
        current_time = datetime.now()
        
        # ä½¿ç”¨promptç®¡ç†å™¨è·å–ç†è§£æŒ‡å¯¼
        understanding_guide = prompt_manager.get_understanding_prompt()
        
        prompt = f"""
        åˆ†æç”¨æˆ·æ¶ˆæ¯å¹¶æå–æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç‰¹åˆ«æ³¨æ„ä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥ã€‚
        
        å½“å‰æ—¶é—´ï¼š{current_time.isoformat()}
        ç”¨æˆ·æ¶ˆæ¯ï¼š{content}
        {context_info}
        {history_context}
        
        {understanding_guide if understanding_guide else ''}
        
        è¯·åˆ†æå¹¶è¿”å›JSONæ ¼å¼çš„ç†è§£ç»“æœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
        1. intent: ç”¨æˆ·æ„å›¾ï¼ˆrecord_expense/record_income/record_health/query/set_reminder/update_info/general_chat/clarification_responseç­‰ï¼‰
        2. entities: æå–çš„å®ä½“ä¿¡æ¯
        3. need_action: æ˜¯å¦éœ€è¦æ‰§è¡ŒåŠ¨ä½œï¼ˆå¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œåº”è¯¥ä¸ºfalseï¼‰
        4. need_clarification: æ˜¯å¦éœ€è¦è¯¢é—®æ›´å¤šä¿¡æ¯ï¼ˆæœ€é‡è¦ï¼ï¼‰
        5. missing_fields: ç¼ºå°‘çš„å…³é”®ä¿¡æ¯å­—æ®µåˆ—è¡¨
        6. clarification_questions: å…·ä½“çš„è¯¢é—®é—®é¢˜åˆ—è¡¨
        7. suggested_actions: å»ºè®®çš„åŠ¨ä½œåˆ—è¡¨
        8. original_content: åŸå§‹æ¶ˆæ¯å†…å®¹ï¼ˆç”¨äºå­˜å‚¨ï¼‰
        9. context_related: æ˜¯å¦ä¸å†å²ä¸Šä¸‹æ–‡ç›¸å…³
        
        **ä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥è§„åˆ™**ï¼š
        - è®°è´¦å¿…éœ€ï¼šé‡‘é¢ã€ç”¨é€”ã€å—ç›Šäººï¼ˆå¦‚æ¶‰åŠå­©å­ï¼‰
        - æé†’å¿…éœ€ï¼šå†…å®¹ã€æ—¶é—´ã€å¯¹è±¡ï¼ˆå¦‚æ¶‰åŠå­©å­ï¼‰
        - å¥åº·è®°å½•å¿…éœ€ï¼šå®¶åº­æˆå‘˜ã€æŒ‡æ ‡ã€æ•°å€¼
        - ä¿¡æ¯æ›´æ–°å¿…éœ€ï¼šæ›´æ–°ç›®æ ‡ã€æ–°ä¿¡æ¯
        
        æ—¶é—´ç†è§£è§„åˆ™ï¼š
        - "ä»Šå¤©" = {current_time.date()}
        - "æ˜¨å¤©" = {(current_time - timedelta(days=1)).date()}
        - "å‰å¤©" = {(current_time - timedelta(days=2)).date()}
        - "ä¸Šå‘¨X" = è®¡ç®—å…·ä½“æ—¥æœŸ
        - "è¿™ä¸ªæœˆ" = {current_time.strftime('%Y-%m')}
        - "ä¸Šä¸ªæœˆ" = è®¡ç®—å…·ä½“æœˆä»½
        
        è´¢åŠ¡ç›¸å…³æå–ï¼š
        - amount: é‡‘é¢ï¼ˆæ•°å­—ï¼‰
        - type: expenseï¼ˆæ”¯å‡ºï¼‰/incomeï¼ˆæ”¶å…¥ï¼‰
        - category: è‡ªåŠ¨åˆ†ç±»ï¼ˆé¤é¥®/è´­ç‰©/äº¤é€š/åŒ»ç–—/æ•™è‚²/è‚²å„¿ç”¨å“/æ—¥ç”¨å“/å¨±ä¹/å…¶ä»–ï¼‰
        - description: å…·ä½“æè¿°
        - person: å¦‚æœæ¶‰åŠç‰¹å®šå®¶åº­æˆå‘˜
        
        å¥åº·ç›¸å…³æå–ï¼š
        - person: å®¶åº­æˆå‘˜ï¼ˆå„¿å­/å¤§å¥³å„¿/äºŒå¥³å„¿/å¦»å­/æˆ‘ï¼‰
        - metric: æŒ‡æ ‡ï¼ˆèº«é«˜/ä½“é‡/ä½“æ¸©/ç–«è‹—/ç—‡çŠ¶ç­‰ï¼‰
        - value: æ•°å€¼
        - unit: å•ä½
        
        æé†’ç›¸å…³æå–ï¼š
        - remind_content: æé†’å†…å®¹
        - remind_time: æé†’æ—¶é—´ï¼ˆè½¬æ¢ä¸ºISOæ ¼å¼ï¼‰
        - repeat: é‡å¤æ¨¡å¼ï¼ˆdaily/weekly/monthly/onceï¼‰
        
        ä¿¡æ¯æ›´æ–°è¯†åˆ«ï¼š
        - å¦‚æœåŒ…å«"æ”¹ä¸º"ã€"æ”¹æˆ"ã€"ç°åœ¨æ˜¯"ã€"æ›´æ–°ä¸º"ç­‰è¯æ±‡ï¼Œè®¾ç½® update_existing: true
        - æå–è¦æ›´æ–°çš„ä¿¡æ¯ç±»å‹å’Œæ–°å€¼
        
        åŸºäºå†å²ä¸Šä¸‹æ–‡çš„ç†è§£ï¼š
        - å¦‚æœæ¶ˆæ¯ä¸­æåˆ°"åˆšæ‰"ã€"ä¸Šé¢"ã€"ä¹‹å‰"ç­‰ï¼Œè¦å…³è”å†å²è®°å½•
        - è¯†åˆ«æ˜¯å¦æ˜¯å¯¹ä¹‹å‰è®°å½•çš„è¡¥å……æˆ–ä¿®æ­£
        
        å¦‚æœç”¨æˆ·åªæ˜¯å›ç­”äº†ä¹‹å‰çš„è¯¢é—®ï¼Œè¯†åˆ«ä¸º clarification_response æ„å›¾ã€‚
        
        è¯·æå–æ‰€æœ‰ä½ è®¤ä¸ºé‡è¦çš„ä¿¡æ¯ï¼Œoccurred_atå­—æ®µå¿…é¡»æ˜¯å…·ä½“çš„ISOæ ¼å¼æ—¶é—´ã€‚
        """
        
        def _safe_json(text: str) -> Dict[str, Any]:
            try:
                return json.loads(text)
            except Exception:
                start = text.find("{")
                end = text.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try:
                        return json.loads(text[start:end+1])
                    except Exception:
                        pass
                return {}

        try:
            understanding = await self.llm.chat_json(
                system_prompt=prompt_manager.get_system_prompt(),
                user_prompt=prompt,
                temperature=0.3,
                max_tokens=1200,
            )
        except Exception as e:
            # æŸäº›å…¼å®¹ç«¯ç‚¹å¯èƒ½ä¸æ”¯æŒ JSON å¼ºçº¦æŸï¼Œé€€åŒ–ä¸ºæ–‡æœ¬å¹¶å°è¯•è§£æ
            logger.warning(f"chat_json failed, fallback to text: {e}")
            raw = await self.llm.chat_text(
                system_prompt=prompt_manager.get_system_prompt(),
                user_prompt=prompt,
                temperature=0.3,
                max_tokens=1200,
            )
            understanding = _safe_json(raw)
        # æ ¡éªŒä¸è¡¥å…¨ç†è§£ç»“æœ
        class UnderstandingModel(BaseModel):
            intent: Optional[str] = None
            entities: Dict[str, Any] = Field(default_factory=dict)
            need_action: bool = False
            need_clarification: bool = False
            missing_fields: List[str] = Field(default_factory=list)
            clarification_questions: List[str] = Field(default_factory=list)
            suggested_actions: List[Dict[str, Any]] = Field(default_factory=list)
            original_content: str = content
            context_related: Optional[bool] = None

        try:
            parsed = UnderstandingModel(**understanding)
            understanding = parsed.model_dump()
        except ValidationError as ve:
            logger.warning("llm.parse.validation_error", trace_id=trace_id, error=str(ve), raw=understanding)
            # å®¹é”™ï¼šæœ€å°‘ä¿è¯å¿…é¡»å­—æ®µå­˜åœ¨
            understanding.setdefault('entities', {})
            understanding.setdefault('need_action', False)
            understanding.setdefault('need_clarification', False)
            understanding.setdefault('missing_fields', [])
            understanding.setdefault('clarification_questions', [])
            understanding.setdefault('suggested_actions', [])

        understanding['original_content'] = content

        logger.info(
            "llm.understanding.response",
            trace_id=trace_id,
            intent=understanding.get('intent'),
            need_action=understanding.get('need_action'),
            need_clarification=understanding.get('need_clarification'),
            entities=understanding.get('entities')
        )
        
        return understanding
    
    async def _build_tool_plan(self, understanding: Dict[str, Any], user_id: str, *, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ç”± LLM äº§å‡ºå·¥å…·è®¡åˆ’ï¼ˆTool Plan/DSLï¼‰ã€‚
        è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
        {
          "steps": [
            {"tool": "store", "args": {"content": "...", "ai_data": {...}}},
            {"tool": "aggregate", "args": {"operation": "sum", "field": "amount", "filters": {...}}}
          ]
        }
        """
        system_prompt = prompt_manager.get_system_prompt()
        planning_guide = prompt_manager.get_tool_planning_prompt()
        user_prompt = (
            (planning_guide or "ä½ å°†ä»¥å·¥å…·ç¼–æ’çš„æ–¹å¼å®Œæˆä»»åŠ¡ã€‚åªè¾“å‡º steps JSONã€‚")
        )
        context_info = {
            "user_id": user_id,
            "channel": (context or {}).get("channel") if context else None,
            "thread_id": (context or {}).get("thread_id") if context else None,
        }
        plan_input = {
            "understanding": understanding,
            "context": context_info,
        }
        try:
            plan = await self.llm.chat_json(
                system_prompt=system_prompt,
                user_prompt=f"è¾“å…¥ï¼š\n{json.dumps(plan_input, ensure_ascii=False)}\n\nè¯·è¾“å‡ºå·¥å…·è®¡åˆ’JSONã€‚\n{user_prompt}",
                temperature=0.2,
                max_tokens=800,
            )
            if not isinstance(plan, dict):
                return {"steps": []}
            steps = plan.get("steps")
            if not isinstance(steps, list):
                return {"steps": []}
            return {"steps": steps}
        except Exception:
            return {"steps": []}
    
    async def _execute_actions(self, understanding: Dict[str, Any], user_id: str, *, context: Optional[Dict[str, Any]] = None, trace_id: str) -> Dict[str, Any]:
        """
        æ ¹æ®ç†è§£ç»“æœæ‰§è¡ŒåŠ¨ä½œ - å¢å¼ºç‰ˆï¼Œæ”¯æŒä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥
        """
        result = {"actions_taken": []}
        
        # ğŸš¨ é‡è¦ï¼šå¦‚æœéœ€è¦æ¾„æ¸…ä¿¡æ¯ï¼Œå…ˆå­˜ä¸€æ¡å¯¹è¯è®°å¿†ç”¨äºå¤šè½®ä¸Šä¸‹æ–‡ï¼ˆä¸æ‰§è¡Œä¸šåŠ¡åŠ¨ä½œï¼‰
        if understanding.get('need_clarification'):
            try:
                entities = understanding.get('entities', {})
                ai_data = {
                    'intent': 'clarification_pending',
                    'entities': entities,
                    'need_clarification': True,
                    'timestamp': datetime.now().isoformat(),
                }
                # ç¡®ä¿æœ‰ occurred_atï¼Œä¾¿äºæ—¶é—´åºæ’åº
                if not ai_data.get('occurred_at'):
                    ai_data['occurred_at'] = datetime.now().isoformat()
                # çº¿ç¨‹/è¿½è¸ªä¿¡æ¯
                if context and context.get('thread_id'):
                    ai_data['thread_id'] = context.get('thread_id')
                ai_data['trace_id'] = trace_id
                # ç”ŸæˆåµŒå…¥
                _emb = None
                try:
                    _embs = await self.llm.embed([understanding.get('original_content', '')])
                    _emb = _embs[0] if _embs else None
                except Exception:
                    _emb = None
                store_result = await self._call_mcp_tool(
                    'store',
                    content=understanding.get('original_content', ''),
                    ai_data=ai_data,
                    user_id=user_id,
                    embedding=_emb
                )
                result['actions_taken'].append({'action': 'store', 'result': store_result})
            except Exception as e:
                logger.error(f"Failed to store clarification context: {e}")
            logger.info("Information incomplete, stored context and waiting for clarification")
            return result
        
        # åªæœ‰ä¿¡æ¯å®Œæ•´æ—¶æ‰å°è¯•æ‰§è¡ŒåŠ¨ä½œï¼ˆç”± LLM å†³å®šæ˜¯å¦éœ€è¦è¡ŒåŠ¨ï¼‰
        if not understanding.get('need_action'):
            return result
        
        # å…ˆè®© LLM äº§å‡ºé€šç”¨å·¥å…·è®¡åˆ’
        plan = await self._build_tool_plan(understanding, user_id, context=context)
        steps = plan.get('steps') or []

        # è‹¥è®¡åˆ’ä¸ºç©ºï¼Œä½œä¸ºå…œåº•ä¸æ‰§è¡Œå…·ä½“åŠ¨ä½œï¼Œä»…è¿”å›
        if not steps:
            return result

        last_store_id: Optional[str] = None
        allowed_tools = {"store", "search", "aggregate", "schedule_reminder", "get_pending_reminders", "mark_reminder_sent"}

        for step in steps:
            if not isinstance(step, dict):
                continue
            tool = step.get('tool')
            args = step.get('args') or {}
            if tool not in allowed_tools:
                continue

            # æ³¨å…¥é€šç”¨å‚æ•°
            if tool in {"store", "search", "aggregate", "get_pending_reminders"}:
                args.setdefault('user_id', user_id)

            # è§£æå ä½ç¬¦ä¾èµ–
            if tool == 'schedule_reminder':
                mem_id = args.get('memory_id')
                if mem_id == '$LAST_STORE_ID' and last_store_id:
                    args['memory_id'] = last_store_id
                if args.get('from_last_store') and last_store_id:
                    args['memory_id'] = last_store_id
                    args.pop('from_last_store', None)

            # ç”ŸæˆåµŒå…¥ï¼šstore.content æˆ– search.query
            try:
                if tool == 'store':
                    text_for_embed = args.get('content') or understanding.get('original_content', '')
                    if text_for_embed:
                        embs = await self.llm.embed([text_for_embed])
                        args.setdefault('embedding', (embs[0] if embs else None))
                    # æœ€ä½é™ä¿éšœï¼šai_data åˆå¹¶ entities ä¸ occurred_at
                    ai_data = args.get('ai_data') or {}
                    entities = understanding.get('entities', {})
                    merged = {**entities, **ai_data}
                    if not merged.get('occurred_at'):
                        merged['occurred_at'] = datetime.now().isoformat()
                    if context and context.get('thread_id'):
                        merged.setdefault('thread_id', context.get('thread_id'))
                    merged.setdefault('trace_id', trace_id)
                    # M1ï¼šå°†é™„ä»¶å…ƒæ•°æ®çº³å…¥å­˜å‚¨ï¼ˆä¾¿äºåç»­æ£€ç´¢ä¸è¿½æº¯ï¼‰
                    if context and isinstance(context.get('attachments'), list):
                        merged.setdefault('attachments', context.get('attachments'))
                    args['ai_data'] = merged
                elif tool == 'search':
                    q = args.get('query')
                    if q and not args.get('query_embedding'):
                        embs = await self.llm.embed([q])
                        args['query_embedding'] = embs[0] if embs else None
                elif tool == 'aggregate':
                    pass
            except Exception:
                # å¿½ç•¥åµŒå…¥å¤±è´¥ï¼Œèµ°æ— åµŒå…¥è·¯å¾„
                pass

            # æ‰§è¡Œå·¥å…·
            exec_result = await self._call_mcp_tool(tool, **args)
            result['actions_taken'].append({'action': tool, 'result': exec_result})

            # è®°å½• last_store_id ä¾›åç»­ä¾èµ–
            if tool == 'store' and isinstance(exec_result, dict) and exec_result.get('success'):
                last_store_id = exec_result.get('id') or last_store_id
        
        return result

    async def _store_chat_turns(
        self,
        *,
        user_id: str,
        thread_id: Optional[str],
        trace_id: str,
        user_message: str,
        assistant_message: str,
        understanding: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> None:
        """å­˜å‚¨ä¸€å¯¹å¯¹è¯å›åˆï¼Œä¾¿äºè¿ç»­å¯¹è¯ä¸å›æº¯ã€‚"""
        common = {
            'type': 'chat_turn',
            'thread_id': thread_id,
            'trace_id': trace_id,
            'channel': (context or {}).get('channel') if context else None,
            'timestamp': datetime.now().isoformat()
        }
        user_ai = {
            **common,
            'role': 'user',
            'intent': understanding.get('intent'),
            'entities': understanding.get('entities', {})
        }
        assistant_ai = {
            **common,
            'role': 'assistant',
            'intent': understanding.get('intent'),
            'entities': understanding.get('entities', {})
        }
        # æ‰¹é‡ç”Ÿæˆä¸¤æ®µæ–‡æœ¬çš„åµŒå…¥
        _user_emb = None
        _assistant_emb = None
        try:
            embs = await self.llm.embed([user_message, assistant_message])
            if embs and len(embs) >= 2:
                _user_emb, _assistant_emb = embs[0], embs[1]
        except Exception:
            _user_emb = None
            _assistant_emb = None
        await self._call_mcp_tool('store', content=user_message, ai_data=user_ai, user_id=user_id, embedding=_user_emb)
        await self._call_mcp_tool('store', content=assistant_message, ai_data=assistant_ai, user_id=user_id, embedding=_assistant_emb)

    async def _maybe_summarize_thread(self, *, user_id: str, thread_id: Optional[str], trace_id: str) -> None:
        """å½“åŒä¸€çº¿ç¨‹å›åˆæ•°è¿‡å¤šæ—¶ï¼Œç”Ÿæˆæ‘˜è¦å¹¶å­˜å‚¨ã€‚"""
        if not thread_id:
            return
        # æ‹‰å–æœ€è¿‘è‹¥å¹²æ¡ï¼Œç­›é€‰å½“çº¿ç¨‹çš„ chat_turn
        recent = await self._call_mcp_tool('search', query='', user_id=user_id, filters={'limit': 50})
        turns = [r for r in recent if isinstance(r, dict) and isinstance(r.get('ai_understanding'), dict) and r['ai_understanding'].get('type') == 'chat_turn' and r['ai_understanding'].get('thread_id') == thread_id]
        if len(turns) < 12:
            return
        # ç”Ÿæˆæ‘˜è¦
        convo_text = "\n".join([f"- {t.get('content','')}" for t in turns[-10:]])
        system_prompt = prompt_manager.get_system_prompt() + "\nè¯·ä¸ºä»¥ä¸Šå¤šè½®å¯¹è¯ç”Ÿæˆç®€æ´æ‘˜è¦ï¼Œä¿ç•™å…³é”®äº‹å®ã€å·²ç¡®è®¤ä¿¡æ¯ä¸æœªå†³é—®é¢˜ã€‚"
        user_prompt = f"éœ€è¦æ‘˜è¦çš„æœ€è¿‘å¯¹è¯ç‰‡æ®µï¼š\n{convo_text}\n\nè¯·è¾“å‡º 5-8 è¡Œçš„è¦ç‚¹åˆ—è¡¨ã€‚"
        summary = await self.llm.chat_text(system_prompt=system_prompt, user_prompt=user_prompt, temperature=0.3, max_tokens=200)
        ai_data = {
            'type': 'thread_summary',
            'thread_id': thread_id,
            'trace_id': trace_id,
            'window': 'last_10_turns',
            'timestamp': datetime.now().isoformat()
        }
        _s_emb = None
        try:
            _s_list = await self.llm.embed([summary])
            _s_emb = _s_list[0] if _s_list else None
        except Exception:
            _s_emb = None
        await self._call_mcp_tool('store', content=summary, ai_data=ai_data, user_id=user_id, embedding=_s_emb)

    async def _get_recent_thread_summaries(self, user_id: str, thread_id: Optional[str], limit: int = 1, *, shared_thread: bool = False, channel: Optional[str] = None) -> List[Dict[str, Any]]:
        filters: Dict[str, Any] = {'limit': 30, 'type': 'thread_summary'}
        if thread_id:
            filters['thread_id'] = thread_id
        if shared_thread:
            filters['shared_thread'] = True
        if channel:
            filters['channel'] = channel
        recent = await self._call_mcp_tool('search', query='thread summary', user_id=user_id, filters=filters)
        summaries: List[Dict[str, Any]] = []
        for r in recent:
            if not isinstance(r, dict):
                continue
            aiu = r.get('ai_understanding')
            if isinstance(aiu, dict) and aiu.get('type') == 'thread_summary' and (thread_id is None or aiu.get('thread_id') == thread_id):
                summaries.append({'content': r.get('content',''), 'ai_understanding': aiu, 'time': r.get('occurred_at')})
        return summaries[:limit]

    async def _semantic_search(self, user_id: str, query: str, top_k: int = 5, *, thread_id: Optional[str] = None, shared_thread: bool = False, channel: Optional[str] = None) -> List[Dict[str, Any]]:
        if not query:
            return []
        filters: Dict[str, Any] = {'limit': top_k}
        if thread_id:
            filters['thread_id'] = thread_id
        if shared_thread:
            filters['shared_thread'] = True
        if channel:
            filters['channel'] = channel
        # ç»Ÿä¸€ç”±å¼•æ“ä¾§ç”ŸæˆæŸ¥è¯¢å‘é‡
        _q_emb = None
        try:
            _q = query if query is not None else ""
            if _q:
                _q_embs = await self.llm.embed([_q])
                _q_emb = _q_embs[0] if _q_embs else None
        except Exception:
            _q_emb = None
        results = await self._call_mcp_tool('search', query=query, user_id=user_id, filters=filters, query_embedding=_q_emb)
        formatted: List[Dict[str, Any]] = []
        for r in results:
            if isinstance(r, dict):
                formatted.append({'content': r.get('content',''), 'ai_understanding': r.get('ai_understanding', {}), 'time': r.get('occurred_at')})
        return formatted

    async def _persist_interaction(
        self,
        *,
        trace_id: str,
        user_id: str,
        thread_id: Optional[str],
        channel: Optional[str],
        message_id: Optional[str],
        input_text: str,
        understanding: Dict[str, Any],
        actions: Dict[str, Any],
        response_text: str,
    ) -> None:
        from .db.database import get_session
        from .db.models import Interaction
        # ä»…å½“ user_id æ˜¯åˆæ³• UUID æ—¶è®°å½•ï¼Œä»¥é¿å…å¤–é”®é”™è¯¯
        if not _looks_like_uuid(user_id):
            logger.warning("interaction.persist.skip.invalid_user_id", user_id=user_id, trace_id=trace_id)
            return
        async with get_session() as session:
            session.add(Interaction(
                id=uuid.UUID(trace_id) if _looks_like_uuid(trace_id) else uuid.uuid4(),
                user_id=uuid.UUID(user_id),
                thread_id=thread_id,
                channel=channel,
                message_id=message_id,
                input_text=input_text,
                understanding_json=understanding,
                actions_json=actions,
                tool_calls_json=None,
                response_text=response_text,
            ))


    
    async def _generate_response(self, original_message: str, understanding: Dict[str, Any], execution_result: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """
        ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤ - å¢å¼ºç‰ˆï¼Œä¼˜å…ˆå¤„ç†ä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥
        """
        # ğŸ¯ ä¼˜å…ˆçº§1ï¼šå¤„ç†ä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µ
        if understanding.get('need_clarification'):
            return await self._generate_clarification_response(original_message, understanding, context)
        
        # ğŸ¯ ä¼˜å…ˆçº§2ï¼šå¤„ç†æ­£å¸¸çš„å®Œæ•´ä¿¡æ¯å›å¤
        return await self._generate_normal_response(original_message, understanding, execution_result, context)
    
    async def _generate_clarification_response(self, original_message: str, understanding: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """
        ç”Ÿæˆæ¾„æ¸…è¯¢é—®çš„å›å¤
        """
        missing_fields = understanding.get('missing_fields', [])
        clarification_questions = understanding.get('clarification_questions', [])
        
        # æ„å»ºæ¸ é“ç‰¹å®šçš„æç¤º
        channel_hint = ""
        if context and context.get('channel') == 'threema':
            channel_hint = "\næ³¨æ„ï¼šé€šè¿‡Threemaå›å¤ï¼Œä¿æŒç®€æ´å‹å¥½ã€‚"
        
        # è·å–å›å¤ç”ŸæˆæŒ‡å¯¼
        response_guide = prompt_manager.get_response_prompt()
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = prompt_manager.get_system_prompt() + f"""

å½“å‰ä»»åŠ¡ï¼šç”¨æˆ·æä¾›çš„ä¿¡æ¯ä¸å®Œæ•´ï¼Œéœ€è¦è¯¢é—®ç¼ºå°‘çš„ä¿¡æ¯ã€‚

{response_guide if response_guide else ''}

è¯¢é—®è¦æ±‚ï¼š
1. ç¡®è®¤å·²ç†è§£çš„éƒ¨åˆ†ä¿¡æ¯
2. ç¤¼è²Œåœ°è¯¢é—®ç¼ºå°‘çš„ä¿¡æ¯
3. æä¾›é€‰æ‹©é€‰é¡¹ï¼ˆå¦‚é€‚ç”¨ï¼‰
4. ä½¿ç”¨æ¸©å’Œã€ä¸“ä¸šçš„è¯­æ°”
5. ä¸€æ¬¡åªè¯¢é—®ä¸€ä¸ªæœ€é‡è¦çš„é—®é¢˜
{channel_hint}

ç¼ºå°‘çš„ä¿¡æ¯ï¼š{', '.join(missing_fields)}
å»ºè®®çš„è¯¢é—®ï¼š{', '.join(clarification_questions)}
"""
        
        # å‡†å¤‡è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        detailed_context = {
            "ç”¨æˆ·æ¶ˆæ¯": original_message,
            "ç†è§£ç»“æœ": understanding,
            "ç¼ºå°‘ä¿¡æ¯": missing_fields,
            "å»ºè®®è¯¢é—®": clarification_questions
        }
        
        prompt = f"""
ç”¨æˆ·æä¾›çš„ä¿¡æ¯ä¸å®Œæ•´ï¼Œéœ€è¦è¯¢é—®ç¼ºå°‘çš„ä¿¡æ¯ã€‚

{json.dumps(detailed_context, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆä¸€ä¸ªæ¸©å’Œã€ä¸“ä¸šçš„è¯¢é—®å›å¤ï¼Œéµå¾ªä»¥ä¸‹æ ¼å¼ï¼š
1. ç¡®è®¤å·²ç†è§£éƒ¨åˆ†ï¼š"å¥½çš„ï¼Œæˆ‘ç†è§£æ‚¨è¦..."
2. ç¤¼è²Œè¯¢é—®ï¼š"è¯·é—®æ‚¨..."
3. æä¾›é€‰æ‹©ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š"æ˜¯...è¿˜æ˜¯...ï¼Ÿ"

è®°ä½è¦åƒå®¶äººä¸€æ ·æ¸©æš–ï¼Œä½†åˆä¿æŒä¸“ä¸šçš„ç²¾ç¡®åº¦ã€‚
"""
        
        return await self.llm.chat_text(
            system_prompt=system_prompt,
            user_prompt=prompt,
            temperature=0.5,
            max_tokens=200,
        )
    
    async def _generate_normal_response(self, original_message: str, understanding: Dict[str, Any], execution_result: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """
        ç”Ÿæˆæ­£å¸¸çš„å›å¤ï¼ˆä¿¡æ¯å®Œæ•´æ—¶ï¼‰
        """
        # æ„å»ºæ‰§è¡Œç»“æœçš„è¯¦ç»†æè¿°
        actions_summary = []
        search_results = []
        aggregation_results = {}
        
        for action in execution_result.get('actions_taken', []):
            action_type = action['action']
            result = action['result']
            
            if action_type == 'store' and result.get('success'):
                actions_summary.append("âœ“ å·²è®°å½•")
            elif action_type == 'schedule_reminder' and result.get('success'):
                actions_summary.append("âœ“ å·²è®¾ç½®æé†’")
            elif action_type == 'search' and isinstance(result, list):
                search_results = result
            elif action_type == 'aggregate' and 'result' in result:
                aggregation_results[result.get('operation', 'sum')] = result['result']
        
        # æ„å»ºæ¸ é“ç‰¹å®šçš„æç¤º
        channel_hint = ""
        if context and context.get('channel') == 'threema':
            channel_hint = "\næ³¨æ„ï¼šé€šè¿‡Threemaå›å¤ï¼Œä¿æŒç®€æ´å‹å¥½ï¼Œä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢åŠ äº²å’ŒåŠ›ã€‚"
        
        # è·å–å›å¤ç”ŸæˆæŒ‡å¯¼
        response_guide = prompt_manager.get_response_prompt()
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = prompt_manager.get_system_prompt() + f"""

å½“å‰ä»»åŠ¡ï¼šåŸºäºç”¨æˆ·æ¶ˆæ¯å’Œæ‰§è¡Œç»“æœï¼Œç”Ÿæˆä¸€ä¸ªæœ‰ä»·å€¼çš„å›å¤ã€‚

{response_guide if response_guide else ''}

å›å¤è¦æ±‚ï¼š
1. ç¡®è®¤å·²å®Œæˆçš„æ“ä½œï¼ˆ{', '.join(actions_summary) if actions_summary else 'æ— æ“ä½œ'}ï¼‰
2. å¦‚æœè®°å½•äº†æ”¯å‡º/æ”¶å…¥ï¼Œè‡ªåŠ¨æä¾›æœ¬æœˆ/ä»Šæ—¥ç´¯è®¡
3. å¦‚æœæ˜¯æŸ¥è¯¢ï¼Œç”¨ç®€æ´çš„æ–¹å¼å±•ç¤ºç»“æœ
4. æ ¹æ®å®¶åº­å†å²æ•°æ®ï¼Œæä¾›ä¸ªæ€§åŒ–å»ºè®®
5. å¦‚æœå‘ç°å¼‚å¸¸æ¨¡å¼ï¼ˆå¦‚è¶…æ”¯ï¼‰ï¼Œæ¸©å’Œæé†’
6. ä½¿ç”¨æ¸©æš–ã€åƒå®¶äººèˆ¬çš„è¯­æ°”
{channel_hint}

è®°ä½è¿™æ˜¯ä¸€ä¸ªæœ‰3ä¸ªå­©å­çš„å®¶åº­ï¼Œå¯èƒ½å…³æ³¨ï¼š
- è‚²å„¿æ”¯å‡ºå’Œå¥åº·
- å®¶åº­é¢„ç®—ç®¡ç†
- æ—¥å¸¸ç”Ÿæ´»ä¾¿åˆ©æ€§
"""
        
        # å‡†å¤‡è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        detailed_context = {
            "ç”¨æˆ·æ¶ˆæ¯": original_message,
            "ç†è§£ç»“æœ": understanding,
            "æ‰§è¡Œæƒ…å†µ": actions_summary,
            "æŸ¥è¯¢ç»“æœæ•°é‡": len(search_results) if search_results else 0,
            "ç»Ÿè®¡ç»“æœ": aggregation_results
        }
        
        # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œæ·»åŠ æ‘˜è¦
        if search_results:
            detailed_context["æœ€è¿‘è®°å½•ç¤ºä¾‹"] = [
                {"å†…å®¹": r.get('content', ''), "é‡‘é¢": r.get('amount')} 
                for r in search_results[:3]
            ]
        
        prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›å¤ï¼š

{json.dumps(detailed_context, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆä¸€ä¸ªç¬¦åˆè¦æ±‚çš„å›å¤ã€‚å¦‚æœæ˜¯è´¢åŠ¡ç›¸å…³ï¼Œè€ƒè™‘ï¼š
- æœ¬æœˆæ€»æ”¯å‡ºæ˜¯å¦å¼‚å¸¸ï¼Ÿ
- æŸç±»æ”¯å‡ºæ˜¯å¦è¿‡é«˜ï¼Ÿ
- æ˜¯å¦éœ€è¦é¢„ç®—æé†’ï¼Ÿ

å¦‚æœæ˜¯å¥åº·ç›¸å…³ï¼Œè€ƒè™‘ï¼š
- æˆé•¿è¶‹åŠ¿æ˜¯å¦æ­£å¸¸ï¼Ÿ
- æ˜¯å¦åˆ°äº†ç–«è‹—æ¥ç§æ—¶é—´ï¼Ÿ
- æ˜¯å¦éœ€è¦å¥åº·å»ºè®®ï¼Ÿ

å¦‚æœæ˜¯æé†’ç›¸å…³ï¼š
- ç¡®è®¤æé†’çš„å…·ä½“æ—¶é—´
- å¦‚æœæ˜¯é‡å¤æé†’ï¼Œè¯´æ˜é¢‘ç‡

è®°ä½è¦åƒå…³å¿ƒå®¶äººä¸€æ ·ï¼Œç»™å‡ºç®€æ´ã€å®ç”¨çš„å»ºè®®ã€‚
"""
        
        generated_response = await self.llm.chat_text(
            system_prompt=system_prompt,
            user_prompt=prompt,
            temperature=0.7,
            max_tokens=500,
        )
        
        # å¦‚æœå­˜åœ¨å¯ç»˜åˆ¶çš„èšåˆåˆ†ç»„ï¼Œæ¸²æŸ“å›¾è¡¨å¹¶è¿½åŠ é“¾æ¥ï¼ˆM2 å›é€€æ–¹æ¡ˆï¼‰
        chart_url: Optional[str] = None
        try:
            for action in execution_result.get('actions_taken', []):
                if action.get('action') == 'aggregate' and isinstance(action.get('result'), dict):
                    groups = action['result'].get('groups')
                    if isinstance(groups, list) and groups:
                        x_labels: List[str] = []
                        y_values: List[float] = []
                        for g in groups:
                            grp = g.get('group') or {}
                            label = grp.get('period') or grp.get('ai_group') or ''
                            if isinstance(label, str):
                                x_labels.append(label)
                            else:
                                x_labels.append(str(label))
                            y_values.append(float(g.get('result') or 0))
                        render_res = await self._call_mcp_tool('render_chart', type='line', title='ç»Ÿè®¡è¶‹åŠ¿', x=x_labels, series=[{"name": "value", "y": y_values}])
                        path = render_res.get('path') if isinstance(render_res, dict) else None
                        if path:
                            chart_url = make_signed_url(path)
                            break
        except Exception:
            pass
        
        # åå¤„ç†ï¼šç¡®ä¿å›å¤ä¸ä¼šå¤ªé•¿
        if len(generated_response) > 500 and context and context.get('channel') == 'threema':
            # å¯¹äºThreemaï¼Œæˆªæ–­è¿‡é•¿çš„æ¶ˆæ¯
            generated_response = generated_response[:497] + "..."
        # è¿½åŠ å›¾è¡¨é“¾æ¥
        if chart_url:
            generated_response += f"\nå›¾è¡¨ï¼š{chart_url}"
        
        return generated_response

    async def generate_chart_and_text(self, *, user_id: str, title: str, x: List[str], series: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        M2ï¼šé€šç”¨å›¾è¡¨æ¸²æŸ“åŒ…è£…ã€‚è¿”å› {text, image_path}ã€‚
        ä¾›æœªæ¥çš„ LLM å·¥å…·è®¡åˆ’æˆ–æ‰‹å·¥è°ƒç”¨ã€‚
        """
        try:
            render = await self._call_mcp_tool('render_chart', type='line', title=title, x=x, series=series)
            image_path = render.get('path') if isinstance(render, dict) else None
            summary = f"{title}: å…± {len(x)} ä¸ªç‚¹ã€‚"
            return {"text": summary, "image_path": image_path}
        except Exception as e:
            return {"text": f"{title}: å›¾è¡¨ç”Ÿæˆå¤±è´¥", "error": str(e)}
    
    async def _call_mcp_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """
        è°ƒç”¨MCPå·¥å…· - ä½¿ç”¨çœŸå®å®¢æˆ·ç«¯æˆ–å›é€€åˆ°æ¨¡æ‹Ÿ
        """
        logger.info(f"Calling MCP tool: {tool_name} with args: {kwargs}")
        
        # å¦‚æœæœ‰çœŸå®çš„MCPå®¢æˆ·ç«¯
        if self.mcp_client:
            try:
                # ä½¿ç”¨httpxè¿›è¡ŒHTTPè°ƒç”¨
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        f"{self.mcp_url}/tool/{tool_name}",
                        json=kwargs,
                        timeout=10.0
                    )
                    response.raise_for_status() # æ£€æŸ¥HTTPçŠ¶æ€ç 
                    return response.json()
            except httpx.RequestError as e:
                logger.error(f"HTTP request to MCP tool failed: {e}")
                # å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼
        
        # æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆç”¨äºå¼€å‘å’Œæµ‹è¯•ï¼‰
        if tool_name == 'store':
            return {"success": True, "id": f"mock-{datetime.now().timestamp()}"}
        elif tool_name == 'search':
            # æ¨¡æ‹Ÿä¸€äº›æœç´¢ç»“æœ
            if "æœ¬æœˆ" in str(kwargs.get('query', '')):
                return [
                    {"content": "ä¹°èœèŠ±äº†50å…ƒ", "amount": 50, "occurred_at": datetime.now().isoformat()},
                    {"content": "æ‰“è½¦èŠ±äº†30å…ƒ", "amount": 30, "occurred_at": datetime.now().isoformat()}
                ]
            return []
        elif tool_name == 'aggregate':
            # æ¨¡æ‹Ÿèšåˆç»“æœ
            if kwargs.get('operation') == 'sum':
                return {"operation": "sum", "field": "amount", "result": 523.5}
            return {"result": 0}
        elif tool_name == 'get_pending_reminders':
            # æ¨¡æ‹Ÿå¾…å‘é€æé†’
            return []
        else:
            return {"success": True}
    
    async def check_and_send_reminders(self, send_callback) -> List[Dict[str, Any]]:
        """
        æ£€æŸ¥å¹¶å‘é€åˆ°æœŸçš„æé†’ - æ”¹è¿›ç‰ˆ
        """
        sent_reminders = []
        
        try:
            # ä»æ•°æ®åº“è·å–æ‰€æœ‰æ´»è·ƒç”¨æˆ·
            from .db.database import get_session
            async with get_session() as db:
                # è·å–æ‰€æœ‰æœ‰ Threema æ¸ é“çš„ç”¨æˆ·
                from sqlalchemy import text
                result = await db.execute(text(
                    """
                    SELECT DISTINCT u.id as user_id
                    FROM users u
                    JOIN user_channels uc ON u.id = uc.user_id
                    WHERE uc.channel = 'threema'
                    """
                ))
                user_rows = result.fetchall()
                
                for row in user_rows:
                    # å…¼å®¹ Row/Mapping è®¿é—®
                    user_id = str(row[0] if isinstance(row, (tuple, list)) else row['user_id'])
                    
                    # è·å–è¯¥ç”¨æˆ·çš„å¾…å‘é€æé†’
                    reminders = await self._call_mcp_tool(
                        'get_pending_reminders',
                        user_id=user_id
                    )
                    
                    for reminder in reminders:
                        # æ„å»ºæé†’æ¶ˆæ¯
                        reminder_content = reminder.get('content', 'æ‚¨è®¾ç½®çš„æé†’')
                        ai_understanding = reminder.get('ai_understanding', {})
                        remind_detail = ai_understanding.get('remind_content', reminder_content)
                        
                        reminder_text = f"â° æé†’ï¼š{remind_detail}\n"
                        if ai_understanding.get('repeat') == 'daily':
                            reminder_text += "ï¼ˆæ¯æ—¥æé†’ï¼‰"
                        
                        # å‘é€æé†’
                        success = await send_callback(user_id, reminder_text)
                        
                        if success:
                            # æ ‡è®°ä¸ºå·²å‘é€
                            await self._call_mcp_tool(
                                'mark_reminder_sent',
                                reminder_id=reminder['reminder_id']
                            )
                            
                            sent_reminders.append({
                                'user_id': user_id,
                                'reminder': reminder,
                                'sent': True
                            })
                            
                            logger.info(f"Sent reminder to user {user_id}: {remind_detail}")
                        else:
                            logger.error(f"Failed to send reminder {reminder['reminder_id']} to {user_id}")
                            
        except Exception as e:
            logger.error(f"Error in reminder task: {e}")
        
        return sent_reminders 