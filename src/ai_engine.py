"""
AIé©±åŠ¨çš„æ ¸å¿ƒå¼•æ“ - è®©AIå†³å®šä¸€åˆ‡
"""
import json
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
import uuid
import time
from pydantic import BaseModel, Field, ValidationError
import openai
import structlog
import httpx
import os

from .core.config import settings
from .core.prompt_manager import prompt_manager
from .core.llm_client import LLMClient
from .core.config import settings
from .services.media_service import make_signed_url

logger = structlog.get_logger(__name__)

def _looks_like_uuid(value: Optional[str]) -> bool:
    if not value or not isinstance(value, str):
        return False
    try:
        uuid.UUID(value)
        return True
    except Exception:
        return False

# ç»Ÿä¸€æ”¹ä¸ºä» YAML è¯»å– promptsï¼ˆè§ prompt_managerï¼‰


class AIEngine:
    def __init__(self):
        # ç»Ÿä¸€ LLM å®¢æˆ·ç«¯ï¼ˆå¯æŒ‰é…ç½®åˆ‡æ¢ OpenAI å…¼å®¹/Anthropic ç­‰ï¼‰
        self.llm = LLMClient()
        self.mcp_client = None
        self.mcp_url = os.getenv('MCP_SERVER_URL', 'http://faa-mcp:8000')
        # å·¥å…·è°ƒç”¨æš‚å­˜ï¼Œç”¨äºäº¤äº’æŒä¹…åŒ–ä¸å›æº¯ï¼ˆæŒ‰ trace_id èšåˆï¼‰
        self._tool_calls_by_trace: Dict[str, List[Dict[str, Any]]] = {}
        # HTTP å®¢æˆ·ç«¯å¤ç”¨
        self._http_client: Optional[httpx.AsyncClient] = None
        # æ¯æ¬¡è¯·æ±‚çº§çš„åµŒå…¥ç¼“å­˜ï¼ˆæŒ‰ trace_id åˆ’åˆ†ï¼‰
        self._emb_cache_by_trace: Dict[str, Dict[str, List[float]]] = {}
        
    async def initialize_mcp(self):
        """åˆå§‹åŒ–MCPå®¢æˆ·ç«¯è¿æ¥"""
        try:
            # æš‚æ—¶ä½¿ç”¨HTTPè°ƒç”¨æ–¹å¼ï¼Œè€Œä¸æ˜¯stdio
            # è¿™æ ·æ›´ç®€å•ä¸”é€‚åˆå®¹å™¨åŒ–éƒ¨ç½²
            logger.info(f"Connecting to MCP server at {self.mcp_url}")
            
            # æµ‹è¯•è¿æ¥ï¼ˆå¤ç”¨ HTTP å®¢æˆ·ç«¯ï¼‰
            if self._http_client is None:
                self._http_client = httpx.AsyncClient()
            response = await self._http_client.get(f"{self.mcp_url}/health", timeout=5.0)
            if response.status_code == 200:
                logger.info("MCP server is healthy")
                self.mcp_client = True  # æ ‡è®°ä¸ºå¯ç”¨
            else:
                logger.warning(f"MCP server health check failed: {response.status_code}")
            
        except Exception as e:
            logger.error(f"Failed to connect to MCP server: {e}")
            logger.warning("Falling back to mock MCP mode")
            self.mcp_client = None
    
    async def close(self):
        """å…³é—­MCPå®¢æˆ·ç«¯è¿æ¥"""
        try:
            if self._http_client is not None:
                await self._http_client.aclose()
        except Exception:
            pass
    
    async def process_message(self, content: str, user_id: str, context: Dict[str, Any] = None) -> str:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯ - å®Œå…¨ç”±AIé©±åŠ¨
        
        Args:
            content: æ¶ˆæ¯å†…å®¹ï¼ˆå·²è§£å¯†çš„æ–‡æœ¬ï¼‰
            user_id: ç”¨æˆ·ID
            context: æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆchannelã€sender_idç­‰ï¼Œè®©AIç†è§£ï¼‰
        """
        # è´¯ç©¿å¼ trace_id
        trace_id = str(uuid.uuid4())
        thread_id = (context or {}).get('thread_id') if context else None
        channel = (context or {}).get('channel') if context else None

        logger.info(
            "message.received",
            trace_id=trace_id,
            user_id=user_id,
            thread_id=thread_id,
            channel=channel,
            message_id=(context or {}).get('message_id') if context else None,
            content_preview=content[:200]
        )

        try:
            # åˆå§‹åŒ–æœ¬æ¬¡ trace çš„å·¥å…·è°ƒç”¨è®°å½•ä¸åµŒå…¥ç¼“å­˜
            self._tool_calls_by_trace[trace_id] = []
            self._emb_cache_by_trace[trace_id] = {}
            # å°†é™„ä»¶çš„è¡ç”Ÿæ–‡æœ¬çº³å…¥å¯æ£€ç´¢/åµŒå…¥çš„è¯­å¢ƒï¼ˆM1ï¼šå…ˆæ‹¼æ¥æ–‡æœ¬ï¼Œä¸åšå¤æ‚æƒé‡ï¼‰
            attachments = (context or {}).get('attachments') if context else None
            derived_texts: List[str] = []
            if isinstance(attachments, list):
                for att in attachments:
                    if not isinstance(att, dict):
                        continue
                    # é¢„ç•™å­—æ®µåï¼Œåç»­å¯ç”±é¢„å¤„ç†æ¨¡å—å¡«å……
                    tx = att.get('transcription', {}).get('text') if isinstance(att.get('transcription'), dict) else None
                    if not tx:
                        tx = att.get('ocr_text')
                    if not tx:
                        tx = att.get('vision_summary')
                    if tx:
                        derived_texts.append(str(tx))
            if derived_texts:
                content = (content or '').strip()
                extra = "\n\n[æ¥è‡ªé™„ä»¶çš„æ–‡æœ¬]\n" + "\n".join(derived_texts)
                content = (content + extra) if content else "\n".join(derived_texts)
            # æ–°æµç¨‹ï¼šå…ˆåšè½»ç†è§£ä»¥ä¾¿æ—©åœ
            understanding = await self._light_understand_message(content, user_id, context, trace_id=trace_id)

            if understanding.get('need_clarification'):
                # ç›´æ¥ç”Ÿæˆæ¾„æ¸…å›å¤ï¼Œçœç•¥æ‰€æœ‰åç»­æ­¥éª¤
                result = {"actions_taken": []}
                response = await self._generate_clarification_response(content, understanding, context)
            else:
                # å¯¹äºæŸ¥è¯¢/åˆ†æç±»ï¼Œå‡çº§ä¸ºé‡ç†è§£ï¼ˆå¸¦æ‰¹é‡ä¸Šä¸‹æ–‡ï¼‰
                do_semantic = self._should_semantic_search(content)
                if do_semantic:
                    understanding = await self._understand_message(content, user_id, context, trace_id=trace_id)
            
                # ç”± LLM äº§å‡ºå·¥å…·è®¡åˆ’
                plan = await self._build_tool_plan(understanding, user_id, context=context)
                steps = plan.get('steps') or []

                # å¦‚æœæœªåšé‡ç†è§£ï¼Œä½†è®¡åˆ’ä¸­å­˜åœ¨ search/aggregateï¼Œåˆ™è¡¥åšä¸€æ¬¡é‡ç†è§£ä»¥æå‡å›ç­”è´¨é‡
                if (not do_semantic) and any(((s or {}).get('tool') in {'search','aggregate'}) for s in steps):
                    understanding = await self._understand_message(content, user_id, context, trace_id=trace_id)
                # æ‰§è¡Œå·¥å…·
                result = await self._execute_tool_steps(steps, understanding, user_id, context=context, trace_id=trace_id)
                # å›å¤ï¼šç®€å•æ“ä½œèµ°å¿«é€Ÿç¡®è®¤ï¼Œå¦åˆ™èµ°æ­£å¸¸ç”Ÿæˆ
                if self._is_simple_actions_only(steps):
                    response = self._build_simple_ack_response(understanding, result, context)
                else:
                    response = await self._generate_normal_response(content, understanding, result, context)

            # å­˜å‚¨å¯¹è¯å›åˆï¼ˆç”¨æˆ·ä¸åŠ©æ‰‹å„ä¸€æ¡ï¼‰ï¼Œç”¨äºè¿ç»­å¯¹è¯ä¸åç»­æ£€ç´¢
            try:
                await self._store_chat_turns(
                    user_id=user_id,
                    thread_id=thread_id,
                    trace_id=trace_id,
                    user_message=content,
                    assistant_message=response,
                    understanding=understanding,
                    context=context,
                )
            except Exception as e:
                logger.error("store.chat_turns.failed", trace_id=trace_id, error=str(e))

            # è§¦å‘ä¼šè¯æ‘˜è¦ï¼ˆåå°ä»»åŠ¡ï¼‰
            try:
                import asyncio as _asyncio
                _asyncio.create_task(self._maybe_summarize_thread(user_id=user_id, thread_id=thread_id, trace_id=trace_id))
            except Exception as e:
                logger.warning("thread.summarize.skip", trace_id=trace_id, error=str(e))

            # è½ç›˜äº¤äº’è½¨è¿¹ï¼Œä¾¿äºæ’éšœ
            try:
                tool_calls = self._tool_calls_by_trace.get(trace_id, [])
                await self._persist_interaction(
                    trace_id=trace_id,
                    user_id=user_id,
                    thread_id=thread_id,
                    channel=channel,
                    message_id=(context or {}).get('message_id') if context else None,
                    input_text=content,
                    understanding=understanding,
                    actions=result,
                    response_text=response,
                    tool_calls=tool_calls,
                )
            except Exception as e:
                logger.error("interaction.persist.failed", trace_id=trace_id, error=str(e))
            
            return response
            
        except Exception as e:
            logger.error("message.process.error", trace_id=trace_id, error=str(e))
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
        finally:
            # æ¸…ç†æœ¬æ¬¡ trace çš„å·¥å…·è°ƒç”¨ç¼“å­˜
            try:
                if trace_id in self._tool_calls_by_trace:
                    self._tool_calls_by_trace.pop(trace_id, None)
                if trace_id in self._emb_cache_by_trace:
                    self._emb_cache_by_trace.pop(trace_id, None)
            except Exception:
                pass
    
    async def _get_recent_memories(self, user_id: str, limit: int = 5, thread_id: Optional[str] = None, *, shared_thread: bool = False, channel: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·æœ€è¿‘çš„äº¤äº’è®°å½•ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£

        Args:
            user_id: å½’å±ç”¨æˆ·
            limit: è¿”å›æ•°é‡ä¸Šé™
            thread_id: çº¿ç¨‹æ ‡è¯†ï¼ˆç”¨äºè¿ç»­ä¸Šä¸‹æ–‡ï¼‰
            shared_thread: æ˜¯å¦æŒ‰çº¿ç¨‹è·¨ç”¨æˆ·å…±äº«æ£€ç´¢
            channel: æ¸ é“ï¼ˆå¦‚ threema/apiï¼‰ï¼Œç”¨äºåœ¨å…±äº«çº¿ç¨‹ä¸‹è¿›ä¸€æ­¥é™å®š
        """
        try:
            # è·å–æœ€è¿‘çš„è®°å¿†
            filters = {'limit': limit}
            if thread_id:
                filters['thread_id'] = thread_id
                filters['type'] = 'chat_turn'
                if shared_thread:
                    filters['shared_thread'] = True
                if channel:
                    filters['channel'] = channel
            recent_memories = await self._call_mcp_tool(
                'search',
                query='',  # ç©ºæŸ¥è¯¢è·å–æœ€æ–°è®°å½•
                user_id=user_id,
                filters=filters
            )
            
            # æ ¼å¼åŒ–è®°å¿†ï¼Œæå–å…³é”®ä¿¡æ¯
            formatted_memories = []
            for memory in recent_memories:
                if isinstance(memory, dict):
                    formatted_memories.append({
                        'content': memory.get('content', ''),
                        'ai_understanding': memory.get('ai_understanding', {}),
                        'time': memory.get('occurred_at', '')
                    })
            
            return formatted_memories
            
        except Exception as e:
            logger.error(f"Error getting recent memories: {e}")
            return []
    
    async def _understand_message(self, content: str, user_id: str, context: Dict[str, Any] = None, *, trace_id: str) -> Dict[str, Any]:
        """
        AIç†è§£æ¶ˆæ¯å†…å®¹ - å¢å¼ºç‰ˆï¼ŒåŒ…å«å†å²ä¸Šä¸‹æ–‡å’Œä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥
        """
        # ä¸Šä¸‹æ–‡æ„å»ºï¼šæ‰¹é‡æ£€ç´¢å‡å°‘ MCP å¾€è¿”
        thread_id = (context or {}).get('thread_id') if context else None
        channel = (context or {}).get('channel') if context else None
        # å…±äº«çº¿ç¨‹ç­–ç•¥ï¼šå½“ä¸Šä¸‹æ–‡æŒ‡ç¤º shared_thread/conversation_scope=shared æ—¶å¯ç”¨
        shared_thread = False
        if context:
            if context.get('shared_thread') is True:
                shared_thread = True
            if context.get('conversation_scope') == 'shared':
                shared_thread = True
        # å¯å‘å¼æ§åˆ¶æ˜¯å¦åšè¯­ä¹‰æ£€ç´¢ï¼Œé¿å…ä¸å¿…è¦çš„ç¬¬äºŒæ¬¡ search
        do_semantic = self._should_semantic_search(content)
        recent_memories, semantic_related, thread_summaries = await self._build_context_via_batch_search(
            user_id=user_id,
            query=content,
            thread_id=thread_id,
            shared_thread=shared_thread,
            channel=channel,
            include_semantic=do_semantic,
            trace_id=trace_id,
        )
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = ""
        if context:
            context_info = f"\næ¶ˆæ¯æ¥æºï¼š{context.get('channel', 'æœªçŸ¥')}"
            if context.get('sender_id'):
                context_info += f"\nå‘é€è€…IDï¼š{context['sender_id']}"
            if context.get('nickname'):
                context_info += f"\nå‘é€è€…æ˜µç§°ï¼š{context['nickname']}"
        
        # åˆ†å—é…é¢ + é‡æ’å»é‡
        def normalize_key(m: Dict[str, Any]) -> str:
            aiu = m.get('ai_understanding') or {}
            intent = aiu.get('intent') if isinstance(aiu, dict) else None
            when = m.get('time') or m.get('occurred_at') or ''
            return f"{m.get('content','')}||{intent or ''}||{when}"

        def fmt_with_budget(items: List[Dict[str, Any]], title: str, char_budget: int, start_index: int = 1, seen: Optional[set] = None) -> Tuple[str, int, int]:
            seen_local = seen if seen is not None else set()
            block_lines: List[str] = []
            count_included = 0
            for idx, m in enumerate(items, start_index):
                key = normalize_key(m)
                if key in seen_local:
                    continue
                seen_local.add(key)
                content_line = m.get('content', '')
                aiu = m.get('ai_understanding') if isinstance(m.get('ai_understanding'), dict) else {}
                intent = aiu.get('intent') if isinstance(aiu, dict) else None
                when = m.get('time') or m.get('occurred_at') or ''
                line = f"\n{idx}. {when}: {content_line}"
                if intent:
                    line += f" (æ„å›¾: {intent})"
                # é¢„ä¼°æ·»åŠ åé•¿åº¦
                projected = (0 if not block_lines else len(''.join(block_lines))) + len(line)
                if projected > char_budget and count_included > 0:
                    break
                block_lines.append(line)
                count_included += 1
            if not block_lines:
                return "", start_index, 0
            header = f"\n\n{title}:"
            text = header + ''.join(block_lines)
            return text, start_index + count_included, count_included

        # é¢„ç®—ï¼šæ€» 3500ï¼ŒæŒ‰ æ‘˜è¦:600 / æœ€è¿‘:2100 / è¯­ä¹‰:800 åˆ†é…
        budget_summary = 600
        budget_recent = 2100
        budget_semantic = 800
        seen_keys: set = set()
        running_index = 1
        history_parts: List[str] = []

        # ä¼˜å…ˆçº§ï¼šæ‘˜è¦ > æœ€è¿‘ > è¯­ä¹‰
        if thread_summaries:
            text, running_index, _ = fmt_with_budget(thread_summaries, "ä¼šè¯æ‘˜è¦", budget_summary, running_index, seen_keys)
            if text:
                history_parts.append(text)
        if recent_memories:
            text, running_index, _ = fmt_with_budget(recent_memories, "æœ€è¿‘çš„äº¤äº’å†å²ï¼ˆç”¨äºç†è§£ä¸Šä¸‹æ–‡ï¼‰", budget_recent, running_index, seen_keys)
            if text:
                history_parts.append(text)
        if semantic_related:
            text, running_index, _ = fmt_with_budget(semantic_related, "ä¸å½“å‰é—®é¢˜è¯­ä¹‰ç›¸å…³çš„å†å²", budget_semantic, running_index, seen_keys)
            if text:
                history_parts.append(text)

        history_context = ''.join(history_parts)
        # å…œåº•å†æˆªæ–­ï¼ˆæå°‘æ•°æƒ…å†µä¸‹ä¸‰å—ä¹‹å’Œä»å¯èƒ½ç•¥è¶…å‡ºï¼‰
        if len(history_context) > 3500:
            history_context = history_context[:3500]

        # æ‰“å°æœ€ç»ˆä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°æ—¥å¿—
        try:
            logger.info(
                "llm.context.built",
                trace_id=trace_id,
                thread_id=thread_id,
                channel=channel,
                shared_thread=shared_thread,
                context_length=len(history_context),
                context_preview=history_context[:500],
                recent_count=len(recent_memories) if isinstance(recent_memories, list) else 0,
                semantic_enabled=do_semantic,
                semantic_count=len(semantic_related) if isinstance(semantic_related, list) else 0,
                summary_count=len(thread_summaries) if isinstance(thread_summaries, list) else 0,
            )
        except Exception:
            pass
        
        # è·å–å½“å‰æ—¶é—´ç”¨äºæ—¶é—´ç†è§£
        current_time = datetime.now()
        
        # ä½¿ç”¨promptç®¡ç†å™¨è·å–ç†è§£æŒ‡å¯¼
        understanding_guide = prompt_manager.get_understanding_prompt()
        
        # æ„å»ºåŠ¨æ€å‚æ•°
        prompt_params = {
            'current_time': current_time.isoformat(),
            'content': content,
            'context_info': context_info,
            'history_context': history_context,
            'understanding_guide': understanding_guide if understanding_guide else '',
            'today_date': current_time.date(),
            'yesterday_date': (current_time - timedelta(days=1)).date(),
            'day_before_yesterday_date': (current_time - timedelta(days=2)).date(),
            'current_month': current_time.strftime('%Y-%m')
        }
        
        # ä» YAML è·å–æ ¼å¼åŒ–çš„ prompt
        prompt = prompt_manager.get_message_understanding_prompt(**prompt_params)
        
        def _safe_json(text: str) -> Dict[str, Any]:
            try:
                return json.loads(text)
            except Exception:
                start = text.find("{")
                end = text.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try:
                        return json.loads(text[start:end+1])
                    except Exception:
                        pass
                return {}

        try:
            # ä½¿ç”¨åŒ…å«åŠ¨æ€å·¥å…·ä¿¡æ¯çš„ç³»ç»Ÿ prompt
            system_prompt_with_tools = await prompt_manager.get_system_prompt_with_tools()
            understanding = await self.llm.chat_json(
                system_prompt=system_prompt_with_tools,
                user_prompt=prompt,
                temperature=0.3,
                max_tokens=1200,
            )
        except Exception as e:
            # æŸäº›å…¼å®¹ç«¯ç‚¹å¯èƒ½ä¸æ”¯æŒ JSON å¼ºçº¦æŸï¼Œé€€åŒ–ä¸ºæ–‡æœ¬å¹¶å°è¯•è§£æ
            logger.warning(f"chat_json failed, fallback to text: {e}")
            raw = await self.llm.chat_text(
                system_prompt=system_prompt_with_tools,
                user_prompt=prompt,
                temperature=0.3,
                max_tokens=1200,
            )
            understanding = _safe_json(raw)
        # æ ¡éªŒä¸è¡¥å…¨ç†è§£ç»“æœ
        class UnderstandingModel(BaseModel):
            intent: Optional[str] = None
            entities: Dict[str, Any] = Field(default_factory=dict)
            need_action: bool = False
            need_clarification: bool = False
            missing_fields: List[str] = Field(default_factory=list)
            clarification_questions: List[str] = Field(default_factory=list)
            suggested_actions: List[Dict[str, Any]] = Field(default_factory=list)
            original_content: str = content
            context_related: Optional[bool] = None

        try:
            parsed = UnderstandingModel(**understanding)
            understanding = parsed.model_dump()
        except ValidationError as ve:
            logger.warning("llm.parse.validation_error", trace_id=trace_id, error=str(ve), raw=understanding)
            # å®¹é”™ï¼šæœ€å°‘ä¿è¯å¿…é¡»å­—æ®µå­˜åœ¨
            understanding.setdefault('entities', {})
            understanding.setdefault('need_action', False)
            understanding.setdefault('need_clarification', False)
            understanding.setdefault('missing_fields', [])
            understanding.setdefault('clarification_questions', [])
            understanding.setdefault('suggested_actions', [])

        understanding['original_content'] = content

        logger.info(
            "llm.understanding.response",
            trace_id=trace_id,
            intent=understanding.get('intent'),
            need_action=understanding.get('need_action'),
            need_clarification=understanding.get('need_clarification'),
            entities=understanding.get('entities')
        )
        
        return understanding

    async def _build_context_via_batch_search(
        self,
        *,
        user_id: str,
        query: str,
        thread_id: Optional[str],
        shared_thread: bool,
        channel: Optional[str],
        include_semantic: bool,
        trace_id: str,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
        """ä½¿ç”¨ batch_search ä¸€æ¬¡å–å› æœ€è¿‘å¯¹è¯/è¯­ä¹‰ç›¸å…³/çº¿ç¨‹æ‘˜è¦ï¼Œå¹¶ç»Ÿä¸€æ ¼å¼åŒ–ã€‚"""
        queries: List[Dict[str, Any]] = []
        # æœ€è¿‘å¯¹è¯
        recent_filters: Dict[str, Any] = {"limit": 10, "type": "chat_turn"}
        if thread_id:
            recent_filters["thread_id"] = thread_id
        if shared_thread:
            recent_filters["shared_thread"] = True
        if channel:
            recent_filters["channel"] = channel
        queries.append({"query": "", "user_id": user_id, "filters": recent_filters})

        # è¯­ä¹‰ç›¸å…³ï¼ˆå¯é€‰ï¼‰
        q_emb: Optional[List[float]] = None
        if include_semantic and query:
            try:
                cache = self._emb_cache_by_trace.get(trace_id, {})
                if query in cache:
                    q_emb = cache[query]
                else:
                    embs = await self.llm.embed([query])
                    q_emb = embs[0] if embs else None
                    cache[query] = q_emb
                    self._emb_cache_by_trace[trace_id] = cache
            except Exception:
                q_emb = None
            sem_filters: Dict[str, Any] = {"limit": 5}
            if thread_id:
                sem_filters["thread_id"] = thread_id
            if shared_thread:
                sem_filters["shared_thread"] = True
            if channel:
                sem_filters["channel"] = channel
            queries.append({"query": query, "user_id": user_id, "filters": sem_filters, "query_embedding": q_emb})

        # çº¿ç¨‹æ‘˜è¦
        summ_filters: Dict[str, Any] = {"limit": 30, "type": "thread_summary"}
        if thread_id:
            summ_filters["thread_id"] = thread_id
        if shared_thread:
            summ_filters["shared_thread"] = True
        if channel:
            summ_filters["channel"] = channel
        queries.append({"query": "thread summary", "user_id": user_id, "filters": summ_filters})

        # æ‰§è¡Œæ‰¹é‡æ£€ç´¢
        batch_res = await self._call_mcp_tool("batch_search", queries=queries, trace_id=trace_id)
        if not isinstance(batch_res, list):
            return [], [], []

        # æ‹†åˆ†å¹¶æ ¼å¼åŒ–
        recent_res = batch_res[0] if len(batch_res) > 0 and isinstance(batch_res[0], list) else []
        sem_res = []
        if include_semantic:
            if len(batch_res) > 1 and isinstance(batch_res[1], list):
                sem_res = batch_res[1]
        summ_res = batch_res[-1] if len(batch_res) >= 1 and isinstance(batch_res[-1], list) else []

        def _fmt_list(src: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            out: List[Dict[str, Any]] = []
            for r in src:
                if isinstance(r, dict) and not r.get("_meta"):
                    out.append({
                        "content": r.get("content", ""),
                        "ai_understanding": r.get("ai_understanding", {}),
                        "time": r.get("occurred_at"),
                    })
            return out

        recent_formatted = _fmt_list(recent_res)
        sem_formatted = _fmt_list(sem_res)

        summ_formatted: List[Dict[str, Any]] = []
        for r in summ_res:
            if not isinstance(r, dict):
                continue
            aiu = r.get("ai_understanding")
            if isinstance(aiu, dict) and aiu.get("type") == "thread_summary" and (thread_id is None or aiu.get("thread_id") == thread_id):
                summ_formatted.append({
                    "content": r.get("content", ""),
                    "ai_understanding": aiu,
                    "time": r.get("occurred_at"),
                })

        return recent_formatted, sem_formatted, (summ_formatted[:1])

    async def _light_understand_message(self, content: str, user_id: str, context: Dict[str, Any] = None, *, trace_id: str) -> Dict[str, Any]:
        """è½»ç†è§£ï¼šä¸åšå†å²æ£€ç´¢ï¼Œä½å¼€é”€åˆ¤å®šæ„å›¾/å®ä½“/æ˜¯å¦éœ€è¦æ¾„æ¸…ã€‚"""
        current_time = datetime.now()
        prompt_params = {
            'current_time': current_time.isoformat(),
            'content': content,
            'context_info': '',
            'history_context': '',
            'understanding_guide': prompt_manager.get_understanding_prompt() or '',
            'today_date': current_time.date(),
            'yesterday_date': (current_time - timedelta(days=1)).date(),
            'day_before_yesterday_date': (current_time - timedelta(days=2)).date(),
            'current_month': current_time.strftime('%Y-%m')
        }
        prompt = prompt_manager.get_message_understanding_prompt(**prompt_params)
        system_prompt_with_tools = await prompt_manager.get_system_prompt_with_tools()
        try:
            understanding = await self.llm.chat_json(
                system_prompt=system_prompt_with_tools,
                user_prompt=prompt,
                temperature=0.2,
                max_tokens=400,
            )
        except Exception:
            raw = await self.llm.chat_text(
                system_prompt=system_prompt_with_tools,
                user_prompt=prompt,
                temperature=0.2,
                max_tokens=400,
            )
            try:
                understanding = json.loads(raw)
            except Exception:
                understanding = {}

        class UnderstandingModel(BaseModel):
            intent: Optional[str] = None
            entities: Dict[str, Any] = Field(default_factory=dict)
            need_action: bool = False
            need_clarification: bool = False
            missing_fields: List[str] = Field(default_factory=list)
            clarification_questions: List[str] = Field(default_factory=list)
            suggested_actions: List[Dict[str, Any]] = Field(default_factory=list)
            original_content: str = content
            context_related: Optional[bool] = None

        try:
            parsed = UnderstandingModel(**understanding)
            understanding = parsed.model_dump()
        except ValidationError as ve:
            logger.warning("llm.light.parse.validation_error", trace_id=trace_id, error=str(ve), raw=understanding)
            understanding.setdefault('entities', {})
            understanding.setdefault('need_action', False)
            understanding.setdefault('need_clarification', False)
            understanding.setdefault('missing_fields', [])
            understanding.setdefault('clarification_questions', [])
            understanding.setdefault('suggested_actions', [])
        understanding['original_content'] = content
        return understanding
    
    async def _build_tool_plan(self, understanding: Dict[str, Any], user_id: str, *, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ç”± LLM äº§å‡ºå·¥å…·è®¡åˆ’ï¼ˆTool Plan/DSLï¼‰ã€‚
        è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
        {
          "steps": [
            {"tool": "store", "args": {"content": "...", "ai_data": {...}}},
            {"tool": "aggregate", "args": {"operation": "sum", "field": "amount", "filters": {...}}}
          ]
        }
        """
        # ä½¿ç”¨åŒ…å«åŠ¨æ€å·¥å…·ä¿¡æ¯çš„ prompts
        system_prompt = await prompt_manager.get_system_prompt_with_tools()
        planning_guide = await prompt_manager.get_tool_planning_prompt_with_tools()
        user_prompt = (
            (planning_guide or "ä½ å°†ä»¥å·¥å…·ç¼–æ’çš„æ–¹å¼å®Œæˆä»»åŠ¡ã€‚åªè¾“å‡º steps JSONã€‚")
        )
        context_info = {
            "user_id": user_id,
            "channel": (context or {}).get("channel") if context else None,
            "thread_id": (context or {}).get("thread_id") if context else None,
        }
        plan_input = {
            "understanding": understanding,
            "context": context_info,
        }
        try:
            plan = await self.llm.chat_json(
                system_prompt=system_prompt,
                user_prompt=f"è¾“å…¥ï¼š\n{json.dumps(plan_input, ensure_ascii=False)}\n\nè¯·è¾“å‡ºå·¥å…·è®¡åˆ’JSONã€‚\n{user_prompt}",
                temperature=0.2,
                max_tokens=800,
            )
            if not isinstance(plan, dict):
                return {"steps": []}
            steps = plan.get("steps")
            if not isinstance(steps, list):
                return {"steps": []}
            return {"steps": steps}
        except Exception:
            return {"steps": []}
    
    def _is_simple_actions_only(self, steps: List[Dict[str, Any]]) -> bool:
        """åªåŒ…å« store/schedule_reminder ç­‰è½»é‡æ“ä½œã€‚"""
        if not steps:
            return True
        for s in steps:
            t = (s or {}).get('tool')
            if t in {"search", "aggregate", "render_chart"}:
                return False
        return True
    
    async def _execute_tool_steps(self, steps: List[Dict[str, Any]], understanding: Dict[str, Any], user_id: str, *, context: Optional[Dict[str, Any]] = None, trace_id: str) -> Dict[str, Any]:
        """æ‰§è¡Œç»™å®šçš„å·¥å…·æ­¥éª¤ï¼›æ”¯æŒåµŒå…¥ç¼“å­˜ä¸æœ€å°åŒ–å‚æ•°æ³¨å…¥ã€‚"""
        result = {"actions_taken": []}
        if not steps:
            return result

        last_store_id: Optional[str] = None
        allowed_tools = {"store", "search", "aggregate", "schedule_reminder", "get_pending_reminders", "mark_reminder_sent", "update_memory_fields", "render_chart", "batch_store", "batch_search"}

        for step in steps:
            if not isinstance(step, dict):
                continue
            tool = step.get('tool')
            args = step.get('args') or {}
            if tool not in allowed_tools:
                continue

            # æ³¨å…¥é€šç”¨å‚æ•°
            if tool in {"store", "search", "aggregate", "get_pending_reminders", "batch_store", "batch_search"}:
                args.setdefault('user_id', user_id)

            # è§£æå ä½ç¬¦ä¾èµ–
            if tool == 'schedule_reminder':
                mem_id = args.get('memory_id')
                if mem_id == '$LAST_STORE_ID' and last_store_id:
                    args['memory_id'] = last_store_id
                if args.get('from_last_store') and last_store_id:
                    args['memory_id'] = last_store_id
                    args.pop('from_last_store', None)

            # ç”ŸæˆåµŒå…¥ï¼šä¼˜å…ˆä»æœ¬æ¬¡ trace çš„ç¼“å­˜å¤ç”¨
            try:
                if tool == 'store':
                    text_for_embed = args.get('content') or understanding.get('original_content', '')
                    if text_for_embed and 'embedding' not in args:
                        cache = self._emb_cache_by_trace.get(trace_id, {})
                        if text_for_embed in cache:
                            args['embedding'] = cache[text_for_embed]
                        else:
                            embs = await self.llm.embed([text_for_embed])
                            # ç¡®ä¿ä¸º Python float åˆ—è¡¨ï¼Œé˜²æ­¢ JSON åºåˆ—åŒ–å¤±è´¥
                            args['embedding'] = ([float(x) for x in embs[0]] if (embs and embs[0]) else None)
                            cache[text_for_embed] = args['embedding']
                            self._emb_cache_by_trace[trace_id] = cache
                    # åˆå¹¶ ai_data
                    ai_data = args.get('ai_data') or {}
                    entities = understanding.get('entities', {})
                    merged = {**entities, **ai_data}
                    if not merged.get('occurred_at'):
                        merged['occurred_at'] = datetime.now().isoformat()
                    if context and context.get('thread_id'):
                        merged.setdefault('thread_id', context.get('thread_id'))
                    merged.setdefault('trace_id', trace_id)
                    if context and isinstance(context.get('attachments'), list):
                        merged.setdefault('attachments', context.get('attachments'))
                    args['ai_data'] = merged
                elif tool == 'search':
                    q = args.get('query')
                    if q and not args.get('query_embedding'):
                        cache = self._emb_cache_by_trace.get(trace_id, {})
                        if q in cache:
                            args['query_embedding'] = cache[q]
                        else:
                            embs = await self.llm.embed([q])
                            args['query_embedding'] = ([float(x) for x in embs[0]] if (embs and embs[0]) else None)
                            cache[q] = args['query_embedding']
                            self._emb_cache_by_trace[trace_id] = cache
            except Exception:
                pass

            exec_result = await self._call_mcp_tool(tool, **{**args, 'trace_id': trace_id})
            result['actions_taken'].append({'action': tool, 'result': exec_result})
            if tool == 'store' and isinstance(exec_result, dict) and exec_result.get('success'):
                last_store_id = exec_result.get('id') or last_store_id
        return result

    async def _store_chat_turns(
        self,
        *,
        user_id: str,
        thread_id: Optional[str],
        trace_id: str,
        user_message: str,
        assistant_message: str,
        understanding: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> None:
        """å­˜å‚¨ä¸€å¯¹å¯¹è¯å›åˆï¼Œä¾¿äºè¿ç»­å¯¹è¯ä¸å›æº¯ã€‚"""
        common = {
            'type': 'chat_turn',
            'thread_id': thread_id,
            'trace_id': trace_id,
            'channel': (context or {}).get('channel') if context else None,
            'timestamp': datetime.now().isoformat()
        }
        user_ai = {**common, 'role': 'user', 'intent': understanding.get('intent'), 'entities': understanding.get('entities', {})}
        assistant_ai = {**common, 'role': 'assistant', 'intent': understanding.get('intent'), 'entities': understanding.get('entities', {})}
        # ä½¿ç”¨æ‰¹é‡å­˜å‚¨ï¼Œä¸”ä¸å¼ºåˆ¶ç”Ÿæˆ embeddingï¼ˆå¯åå°é‡åµŒï¼‰
        memories = [
            {"content": user_message, "ai_data": user_ai, "user_id": user_id},
            {"content": assistant_message, "ai_data": assistant_ai, "user_id": user_id},
        ]
        await self._call_mcp_tool('batch_store', memories=memories, trace_id=trace_id)

    async def _maybe_summarize_thread(self, *, user_id: str, thread_id: Optional[str], trace_id: str) -> None:
        """å½“åŒä¸€çº¿ç¨‹å›åˆæ•°è¿‡å¤šæ—¶ï¼Œç”Ÿæˆæ‘˜è¦å¹¶å­˜å‚¨ã€‚"""
        if not thread_id:
            return
        # æ‹‰å–æœ€è¿‘è‹¥å¹²æ¡ï¼Œç­›é€‰å½“çº¿ç¨‹çš„ chat_turnï¼ˆåŠ ç²¾ç¡®è¿‡æ»¤ï¼Œå‡å°‘æ‰«æï¼‰
        recent = await self._call_mcp_tool('search', query='', user_id=user_id, filters={'limit': 50, 'type': 'chat_turn', 'thread_id': thread_id}, trace_id=trace_id)
        turns = [r for r in recent if isinstance(r, dict)]
        if len(turns) < 12:
            return
        # ç”Ÿæˆæ‘˜è¦
        convo_text = "\n".join([f"- {t.get('content','')}" for t in turns[-10:]])
        system_prompt = prompt_manager.get_system_prompt() + "\nè¯·ä¸ºä»¥ä¸Šå¤šè½®å¯¹è¯ç”Ÿæˆç®€æ´æ‘˜è¦ï¼Œä¿ç•™å…³é”®äº‹å®ã€å·²ç¡®è®¤ä¿¡æ¯ä¸æœªå†³é—®é¢˜ã€‚"
        user_prompt = f"éœ€è¦æ‘˜è¦çš„æœ€è¿‘å¯¹è¯ç‰‡æ®µï¼š\n{convo_text}\n\nè¯·è¾“å‡º 5-8 è¡Œçš„è¦ç‚¹åˆ—è¡¨ã€‚"
        summary = await self.llm.chat_text(system_prompt=system_prompt, user_prompt=user_prompt, temperature=0.3, max_tokens=200)
        ai_data = {
            'type': 'thread_summary',
            'thread_id': thread_id,
            'trace_id': trace_id,
            'window': 'last_10_turns',
            'timestamp': datetime.now().isoformat()
        }
        _s_emb = None
        try:
            _s_list = await self.llm.embed([summary])
            _s_emb = _s_list[0] if _s_list else None
        except Exception:
            _s_emb = None
        await self._call_mcp_tool('store', content=summary, ai_data=ai_data, user_id=user_id, embedding=_s_emb)

    async def _get_recent_thread_summaries(self, user_id: str, thread_id: Optional[str], limit: int = 1, *, shared_thread: bool = False, channel: Optional[str] = None) -> List[Dict[str, Any]]:
        filters: Dict[str, Any] = {'limit': 30, 'type': 'thread_summary'}
        if thread_id:
            filters['thread_id'] = thread_id
        if shared_thread:
            filters['shared_thread'] = True
        if channel:
            filters['channel'] = channel
        recent = await self._call_mcp_tool('search', query='thread summary', user_id=user_id, filters=filters)
        summaries: List[Dict[str, Any]] = []
        for r in recent:
            if not isinstance(r, dict):
                continue
            aiu = r.get('ai_understanding')
            if isinstance(aiu, dict) and aiu.get('type') == 'thread_summary' and (thread_id is None or aiu.get('thread_id') == thread_id):
                summaries.append({'content': r.get('content',''), 'ai_understanding': aiu, 'time': r.get('occurred_at')})
        return summaries[:limit]

    async def _semantic_search(self, user_id: str, query: str, top_k: int = 5, *, thread_id: Optional[str] = None, shared_thread: bool = False, channel: Optional[str] = None) -> List[Dict[str, Any]]:
        if not query:
            return []
        filters: Dict[str, Any] = {'limit': top_k}
        if thread_id:
            filters['thread_id'] = thread_id
        if shared_thread:
            filters['shared_thread'] = True
        if channel:
            filters['channel'] = channel
        # ç»Ÿä¸€ç”±å¼•æ“ä¾§ç”ŸæˆæŸ¥è¯¢å‘é‡
        _q_emb = None
        try:
            _q = query if query is not None else ""
            if _q:
                _q_embs = await self.llm.embed([_q])
                _q_emb = _q_embs[0] if _q_embs else None
        except Exception:
            _q_emb = None
        results = await self._call_mcp_tool('search', query=query, user_id=user_id, filters=filters, query_embedding=_q_emb)
        formatted: List[Dict[str, Any]] = []
        for r in results:
            if isinstance(r, dict):
                formatted.append({'content': r.get('content',''), 'ai_understanding': r.get('ai_understanding', {}), 'time': r.get('occurred_at')})
        return formatted

    async def _persist_interaction(
        self,
        *,
        trace_id: str,
        user_id: str,
        thread_id: Optional[str],
        channel: Optional[str],
        message_id: Optional[str],
        input_text: str,
        understanding: Dict[str, Any],
        actions: Dict[str, Any],
        response_text: str,
        tool_calls: Optional[List[Dict[str, Any]]] = None,
    ) -> None:
        from .db.database import get_session
        from .db.models import Interaction
        # ä»…å½“ user_id æ˜¯åˆæ³• UUID æ—¶è®°å½•ï¼Œä»¥é¿å…å¤–é”®é”™è¯¯
        if not _looks_like_uuid(user_id):
            logger.warning("interaction.persist.skip.invalid_user_id", user_id=user_id, trace_id=trace_id)
            return
        async with get_session() as session:
            session.add(Interaction(
                id=uuid.UUID(trace_id) if _looks_like_uuid(trace_id) else uuid.uuid4(),
                user_id=uuid.UUID(user_id),
                thread_id=thread_id,
                channel=channel,
                message_id=message_id,
                input_text=input_text,
                understanding_json=understanding,
                actions_json=actions,
                tool_calls_json=tool_calls or [],
                response_text=response_text,
            ))


    
    async def _generate_response(self, original_message: str, understanding: Dict[str, Any], execution_result: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """
        ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤ - å¢å¼ºç‰ˆï¼Œä¼˜å…ˆå¤„ç†ä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥
        """
        # ğŸ¯ ä¼˜å…ˆçº§1ï¼šå¤„ç†ä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µ
        if understanding.get('need_clarification'):
            return await self._generate_clarification_response(original_message, understanding, context)
        
        # ğŸ¯ ä¼˜å…ˆçº§2ï¼šå¤„ç†æ­£å¸¸çš„å®Œæ•´ä¿¡æ¯å›å¤
        return await self._generate_normal_response(original_message, understanding, execution_result, context)

    def _build_simple_ack_response(self, understanding: Dict[str, Any], execution_result: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> str:
        """ç®€å•ç¡®è®¤ç±»å›å¤ï¼ˆæ— éœ€å†æ¬¡è°ƒç”¨ LLMï¼‰ã€‚"""
        actions = execution_result.get('actions_taken') or []
        ok = any((a.get('action') == 'store' and isinstance(a.get('result'), dict) and a['result'].get('success')) for a in actions)
        intent = understanding.get('intent') or ''
        entities = understanding.get('entities') or {}
        key_bits: List[str] = []
        # æå–ä¸€äº›å¸¸è§å­—æ®µå½¢æˆç®€çŸ­å›æ˜¾
        for k in ['amount', 'occurred_at', 'category', 'person', 'value', 'unit', 'item']:
            v = entities.get(k)
            if v is not None:
                key_bits.append(f"{k}={v}")
        echo = ("ï¼Œ".join(key_bits)) if key_bits else ""
        prefix = "å·²è®°å½•" if ok else "å·²å¤„ç†"
        if intent:
            text = f"{prefix}ï¼ˆ{intent}ï¼‰"
        else:
            text = prefix
        if echo:
            text += f"ï¼š{echo}"
        return text
    
    async def _generate_clarification_response(self, original_message: str, understanding: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """
        ç”Ÿæˆæ¾„æ¸…è¯¢é—®çš„å›å¤
        """
        missing_fields = understanding.get('missing_fields', [])
        clarification_questions = understanding.get('clarification_questions', [])
        
        # æ„å»ºæ¸ é“ç‰¹å®šçš„æç¤º
        channel_hint = ""
        if context and context.get('channel') == 'threema':
            channel_hint = "\næ³¨æ„ï¼šé€šè¿‡Threemaå›å¤ï¼Œä¿æŒç®€æ´å‹å¥½ã€‚"
        
        # è·å–å›å¤ç”ŸæˆæŒ‡å¯¼ï¼ˆæ¾„æ¸…ä¸“ç”¨ä¸é€šç”¨ï¼‰
        response_guide = prompt_manager.get_response_prompt() or ''
        # è‹¥å®šä¹‰äº†ä¸“é—¨çš„æ¾„æ¸…å—ï¼Œé™„åŠ è¿›ç³»ç»Ÿæç¤º
        clar_block = ''
        try:
            clar_block = prompt_manager.prompts.get(prompt_manager.current_version, {}).get('response_clarification', '')
        except Exception:
            clar_block = ''
        
        # ä½¿ç”¨åŠ¨æ€ task prompt
        task_params = {
            'channel_hint': channel_hint,
            'missing_fields': ', '.join(missing_fields),
            'clarification_questions': ', '.join(clarification_questions)
        }
        task_prompt = prompt_manager.get_clarification_task_prompt(**task_params)
        
        # æ„å»ºç³»ç»Ÿæç¤ºï¼ˆä½¿ç”¨åŠ¨æ€å·¥å…·ä¿¡æ¯ï¼‰
        base_system_prompt = await prompt_manager.get_system_prompt_with_tools()
        system_prompt = base_system_prompt + ("\n" + clar_block if clar_block else "") + f"\n\n{response_guide if response_guide else ''}\n\n{task_prompt}"
        
        # å‡†å¤‡è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        detailed_context = {
            "ç”¨æˆ·æ¶ˆæ¯": original_message,
            "ç†è§£ç»“æœ": understanding,
            "ç¼ºå°‘ä¿¡æ¯": missing_fields,
            "å»ºè®®è¯¢é—®": clarification_questions
        }
        
        # ä½¿ç”¨åŠ¨æ€ç”¨æˆ·æç¤º
        user_params = {
            'detailed_context': json.dumps(detailed_context, ensure_ascii=False, indent=2)
        }
        prompt = prompt_manager.get_clarification_user_prompt(**user_params)
        
        return await self.llm.chat_text(
            system_prompt=system_prompt,
            user_prompt=prompt,
            temperature=0.5,
            max_tokens=200,
        )
    
    async def _generate_normal_response(self, original_message: str, understanding: Dict[str, Any], execution_result: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """
        ç”Ÿæˆæ­£å¸¸çš„å›å¤ï¼ˆä¿¡æ¯å®Œæ•´æ—¶ï¼‰
        """
        # æ„å»ºæ‰§è¡Œç»“æœçš„è¯¦ç»†æè¿°
        actions_summary = []
        search_results = []
        aggregation_results = {}
        
        for action in execution_result.get('actions_taken', []):
            action_type = action['action']
            result = action['result']
            
            if action_type == 'store' and result.get('success'):
                actions_summary.append("âœ“ å·²è®°å½•")
            elif action_type == 'schedule_reminder' and result.get('success'):
                actions_summary.append("âœ“ å·²è®¾ç½®æé†’")
            elif action_type == 'search' and isinstance(result, list):
                search_results = result
            elif action_type == 'aggregate' and 'result' in result:
                aggregation_results[result.get('operation', 'sum')] = result['result']
        
        # æ„å»ºæ¸ é“ç‰¹å®šçš„æç¤º
        channel_hint = ""
        if context and context.get('channel') == 'threema':
            channel_hint = "\næ³¨æ„ï¼šé€šè¿‡Threemaå›å¤ï¼Œä¿æŒç®€æ´å‹å¥½ï¼Œä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢åŠ äº²å’ŒåŠ›ã€‚"
        
        # è·å–å›å¤ç”ŸæˆæŒ‡å¯¼ï¼ˆæ­£å¸¸å›å¤ï¼‰
        response_guide = prompt_manager.get_response_prompt() or ''
        normal_block = ''
        try:
            normal_block = prompt_manager.prompts.get(prompt_manager.current_version, {}).get('response_normal', '')
        except Exception:
            normal_block = ''
        
        # ä½¿ç”¨åŠ¨æ€ task prompt
        task_params = {
            'channel_hint': channel_hint,
            'actions_summary': ', '.join(actions_summary) if actions_summary else 'æ— æ“ä½œ'
        }
        task_prompt = prompt_manager.get_normal_task_prompt(**task_params)
        
        # æ„å»ºç³»ç»Ÿæç¤ºï¼ˆä½¿ç”¨åŠ¨æ€å·¥å…·ä¿¡æ¯ï¼‰
        base_system_prompt = await prompt_manager.get_system_prompt_with_tools()
        system_prompt = base_system_prompt + ("\n" + normal_block if normal_block else "") + f"\n\n{response_guide if response_guide else ''}\n\n{task_prompt}"
        
        # å‡†å¤‡è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        detailed_context = {
            "ç”¨æˆ·æ¶ˆæ¯": original_message,
            "ç†è§£ç»“æœ": understanding,
            "æ‰§è¡Œæƒ…å†µ": actions_summary,
            "æŸ¥è¯¢ç»“æœæ•°é‡": len(search_results) if search_results else 0,
            "ç»Ÿè®¡ç»“æœ": aggregation_results
        }
        
        # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œæ·»åŠ æ‘˜è¦
        if search_results:
            detailed_context["æœ€è¿‘è®°å½•ç¤ºä¾‹"] = [
                {"å†…å®¹": r.get('content', ''), "é‡‘é¢": r.get('amount')} 
                for r in search_results[:3]
            ]
        
        # ä½¿ç”¨åŠ¨æ€ç”¨æˆ·æç¤º
        user_params = {
            'detailed_context': json.dumps(detailed_context, ensure_ascii=False, indent=2)
        }
        prompt = prompt_manager.get_normal_user_prompt(**user_params)
        
        generated_response = await self.llm.chat_text(
            system_prompt=system_prompt,
            user_prompt=prompt,
            temperature=0.7,
            max_tokens=500,
        )
        
        # å¦‚æœå­˜åœ¨å¯ç»˜åˆ¶çš„èšåˆåˆ†ç»„ï¼Œæ¸²æŸ“å›¾è¡¨å¹¶è¿½åŠ é“¾æ¥ï¼ˆM2 å›é€€æ–¹æ¡ˆï¼‰
        chart_url: Optional[str] = None
        try:
            for action in execution_result.get('actions_taken', []):
                if action.get('action') == 'aggregate' and isinstance(action.get('result'), dict):
                    groups = action['result'].get('groups')
                    if isinstance(groups, list) and groups:
                        x_labels: List[str] = []
                        y_values: List[float] = []
                        for g in groups:
                            grp = g.get('group') or {}
                            label = grp.get('period') or grp.get('ai_group') or ''
                            if isinstance(label, str):
                                x_labels.append(label)
                            else:
                                x_labels.append(str(label))
                            y_values.append(float(g.get('result') or 0))
                        render_res = await self._call_mcp_tool('render_chart', type='line', title='ç»Ÿè®¡è¶‹åŠ¿', x=x_labels, series=[{"name": "value", "y": y_values}])
                        path = render_res.get('path') if isinstance(render_res, dict) else None
                        if path:
                            chart_url = make_signed_url(path)
                            break
        except Exception:
            pass
        
        # åå¤„ç†ï¼šç¡®ä¿å›å¤ä¸ä¼šå¤ªé•¿
        if len(generated_response) > 500 and context and context.get('channel') == 'threema':
            # å¯¹äºThreemaï¼Œæˆªæ–­è¿‡é•¿çš„æ¶ˆæ¯
            generated_response = generated_response[:497] + "..."
        # è¿½åŠ å›¾è¡¨é“¾æ¥
        if chart_url:
            generated_response += f"\nå›¾è¡¨ï¼š{chart_url}"
        
        return generated_response

    async def generate_chart_and_text(self, *, user_id: str, title: str, x: List[str], series: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        M2ï¼šé€šç”¨å›¾è¡¨æ¸²æŸ“åŒ…è£…ã€‚è¿”å› {text, image_path}ã€‚
        ä¾›æœªæ¥çš„ LLM å·¥å…·è®¡åˆ’æˆ–æ‰‹å·¥è°ƒç”¨ã€‚
        """
        try:
            render = await self._call_mcp_tool('render_chart', type='line', title=title, x=x, series=series)
            image_path = render.get('path') if isinstance(render, dict) else None
            summary = f"{title}: å…± {len(x)} ä¸ªç‚¹ã€‚"
            return {"text": summary, "image_path": image_path}
        except Exception as e:
            return {"text": f"{title}: å›¾è¡¨ç”Ÿæˆå¤±è´¥", "error": str(e)}
    
    async def _call_mcp_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """
        è°ƒç”¨MCPå·¥å…· - ä½¿ç”¨çœŸå®å®¢æˆ·ç«¯æˆ–å›é€€åˆ°æ¨¡æ‹Ÿ
        """
        trace_id = kwargs.get('trace_id')

        # æ¸…ç†ä¸æ¦‚æ‹¬å‚æ•°ï¼Œé¿å…æ—¥å¿—ä¸­å‡ºç°å¤§ä½“ç§¯å‘é‡
        def _summarize_args(d: Dict[str, Any]) -> Dict[str, Any]:
            if not isinstance(d, dict):
                return {}
            out: Dict[str, Any] = {}
            for k, v in d.items():
                if k in {"embedding", "query_embedding"}:
                    try:
                        dim = len(v) if isinstance(v, (list, tuple)) else None
                        out[k] = f"[vector {dim} dims]"
                    except Exception:
                        out[k] = "[vector]"
                elif k in {"content", "query"} and isinstance(v, str):
                    out[f"{k}_len"] = len(v)
                    out[f"{k}_preview"] = v[:80]
                else:
                    out[k] = v
            return out

        log_args = _summarize_args(kwargs)
        start_ts = time.perf_counter()
        logger.info(
            "mcp.tool.call.start",
            trace_id=trace_id,
            tool=tool_name,
            args=log_args,
        )

        http_status = None
        result_json: Any = None
        # å¦‚æœæœ‰çœŸå®çš„MCPå®¢æˆ·ç«¯
        if self.mcp_client:
            try:
                # ä½¿ç”¨httpxè¿›è¡ŒHTTPè°ƒç”¨ï¼ˆå¤ç”¨å®¢æˆ·ç«¯ï¼‰
                if self._http_client is None:
                    self._http_client = httpx.AsyncClient()
                response = await self._http_client.post(
                        f"{self.mcp_url}/tool/{tool_name}",
                        json=kwargs,
                        timeout=10.0
                    )
                http_status = response.status_code
                response.raise_for_status() # æ£€æŸ¥HTTPçŠ¶æ€ç 
                result_json = response.json()
            except httpx.RequestError as e:
                logger.error(f"HTTP request to MCP tool failed: {e}")
                # å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼
        
        # æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆç”¨äºå¼€å‘å’Œæµ‹è¯•ï¼‰æˆ– HTTP è¿”å›å¤±è´¥æ—¶çš„å…œåº•
        if result_json is None:
            if tool_name == 'store':
                result_json = {"success": True, "id": f"mock-{datetime.now().timestamp()}"}
            elif tool_name == 'search':
                # æ¨¡æ‹Ÿä¸€äº›æœç´¢ç»“æœ
                if "æœ¬æœˆ" in str(kwargs.get('query', '')):
                    result_json = [
                        {"content": "ä¹°èœèŠ±äº†50å…ƒ", "amount": 50, "occurred_at": datetime.now().isoformat()},
                        {"content": "æ‰“è½¦èŠ±äº†30å…ƒ", "amount": 30, "occurred_at": datetime.now().isoformat()}
                    ]
                else:
                    result_json = []
            elif tool_name == 'aggregate':
                # æ¨¡æ‹Ÿèšåˆç»“æœ
                if kwargs.get('operation') == 'sum':
                    result_json = {"operation": "sum", "field": "amount", "result": 523.5}
                else:
                    result_json = {"result": 0}
            elif tool_name == 'get_pending_reminders':
                # æ¨¡æ‹Ÿå¾…å‘é€æé†’
                result_json = []
            else:
                result_json = {"success": True}

        # ç»“æŸæ—¥å¿—ä¸è°ƒç”¨è®°å½•
        try:
            duration_ms = int((time.perf_counter() - start_ts) * 1000)
            if isinstance(result_json, list):
                result_count = len(result_json)
            elif isinstance(result_json, dict):
                result_count = 1
            else:
                result_count = 0
            logger.info(
                "mcp.tool.call.end",
                trace_id=trace_id,
                tool=tool_name,
                http_status=http_status,
                duration_ms=duration_ms,
                result_count=result_count,
            )
            # å°†è°ƒç”¨è®°å½•å…¥å†…å­˜ï¼Œä¾¿äºæŒä¹…åŒ–
            if trace_id:
                rec = {
                    'tool': tool_name,
                    'args': log_args,
                    'http_status': http_status,
                    'duration_ms': duration_ms,
                    'result_count': result_count,
                    'ts': datetime.now().isoformat(),
                }
                self._tool_calls_by_trace.setdefault(trace_id, []).append(rec)
        except Exception:
            pass

        return result_json

    def _should_semantic_search(self, content: str) -> bool:
        """å¯å‘å¼ï¼šä»…åœ¨ç–‘ä¼¼æŸ¥è¯¢/ç»Ÿè®¡/å›é¡¾ç±»æˆ–è¾ƒå¤æ‚æ–‡æœ¬æ—¶å¯ç”¨è¯­ä¹‰æ£€ç´¢ã€‚"""
        if not content:
            return False
        text = str(content)
        # é—®å¥æˆ–ä¿¡æ¯æ£€ç´¢è¯
        query_markers = ["?", "ï¼Ÿ", "æŸ¥è¯¢", "æŸ¥", "æŸ¥çœ‹", "çœ‹çœ‹", "ç»Ÿè®¡", "å¤šå°‘", "æ€»å…±", "åˆè®¡", "å†å²", "æœ€è¿‘", "ä»¥å‰", "ä¸Šæ¬¡", "å¯¹æ¯”", "è¶‹åŠ¿"]
        if any(m in text for m in query_markers):
            return True
        # æ–‡æœ¬è¾ƒé•¿æ—¶ä¹Ÿå¯ç”¨ï¼ˆéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ç†è§£ï¼‰
        return len(text) >= 40
    
    async def check_and_send_reminders(self, send_callback) -> List[Dict[str, Any]]:
        """
        æ£€æŸ¥å¹¶å‘é€åˆ°æœŸçš„æé†’ - æ”¹è¿›ç‰ˆ
        """
        sent_reminders = []
        
        try:
            # ä»æ•°æ®åº“è·å–æ‰€æœ‰æ´»è·ƒç”¨æˆ·
            from .db.database import get_session
            async with get_session() as db:
                # è·å–æ‰€æœ‰æœ‰ Threema æ¸ é“çš„ç”¨æˆ·
                from sqlalchemy import text
                result = await db.execute(text(
                    """
                    SELECT DISTINCT u.id as user_id
                    FROM users u
                    JOIN user_channels uc ON u.id = uc.user_id
                    WHERE uc.channel = 'threema'
                    """
                ))
                user_rows = result.fetchall()
                
                for row in user_rows:
                    # å…¼å®¹ Row/Mapping è®¿é—®
                    user_id = str(row[0] if isinstance(row, (tuple, list)) else row['user_id'])
                    
                    # è·å–è¯¥ç”¨æˆ·çš„å¾…å‘é€æé†’
                    reminders = await self._call_mcp_tool(
                        'get_pending_reminders',
                        user_id=user_id
                    )
                    
                    for reminder in reminders:
                        # æ„å»ºæé†’æ¶ˆæ¯
                        reminder_content = reminder.get('content', 'æ‚¨è®¾ç½®çš„æé†’')
                        ai_understanding = reminder.get('ai_understanding', {})
                        remind_detail = ai_understanding.get('remind_content', reminder_content)
                        
                        reminder_text = f"â° æé†’ï¼š{remind_detail}\n"
                        if ai_understanding.get('repeat') == 'daily':
                            reminder_text += "ï¼ˆæ¯æ—¥æé†’ï¼‰"
                        
                        # å‘é€æé†’
                        success = await send_callback(user_id, reminder_text)
                        
                        if success:
                            # æ ‡è®°ä¸ºå·²å‘é€
                            await self._call_mcp_tool(
                                'mark_reminder_sent',
                                reminder_id=reminder['reminder_id']
                            )
                            
                            sent_reminders.append({
                                'user_id': user_id,
                                'reminder': reminder,
                                'sent': True
                            })
                            
                            logger.info(f"Sent reminder to user {user_id}: {remind_detail}")
                        else:
                            logger.error(f"Failed to send reminder {reminder['reminder_id']} to {user_id}")
                            
        except Exception as e:
            logger.error(f"Error in reminder task: {e}")
        
        return sent_reminders 