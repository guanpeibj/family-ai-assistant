"""
A/B æµ‹è¯•æ¡†æ¶ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ FAA ä¸­è®¾ç½®å’Œç®¡ç† A/B æµ‹è¯•ï¼Œ
ç”¨äºå®‰å…¨åœ°æµ‹è¯•æ–°çš„ Prompt ç‰ˆæœ¬å’Œ AI è¡Œä¸ºæ¨¡å¼ã€‚

ä½¿ç”¨åœºæ™¯ï¼š
1. æµ‹è¯•æ–°çš„å¯¹è¯é£æ ¼ï¼ˆæ›´å‹å–„ vs æ›´ä¸“ä¸šï¼‰
2. æµ‹è¯•ä¸åŒçš„æ¾„æ¸…ç­–ç•¥ï¼ˆä¸»åŠ¨æ¾„æ¸… vs æ™ºèƒ½æ¨æ–­ï¼‰
3. æµ‹è¯•æ–°çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼ˆä¿å®ˆ vs ç§¯æä½¿ç”¨å·¥å…·ï¼‰
"""
import asyncio
from datetime import datetime, timedelta
from src.core.ab_testing import (
    ABTestingManager, ExperimentConfig, ExperimentStatus, 
    ExperimentResult, get_experiment_version
)

# åˆå§‹åŒ– A/B æµ‹è¯•ç®¡ç†å™¨
ab_manager = ABTestingManager()


def create_conversational_style_experiment():
    """ç¤ºä¾‹1: æµ‹è¯•å¯¹è¯é£æ ¼çš„ A/B å®éªŒ"""
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = ExperimentConfig(
        id="conversation_style_v1",
        name="å¯¹è¯é£æ ¼æµ‹è¯•ï¼šå‹å–„ vs ä¸“ä¸š",
        description="æµ‹è¯•æ›´å‹å–„çš„å¯¹è¯é£æ ¼æ˜¯å¦èƒ½æå‡ç”¨æˆ·æ»¡æ„åº¦",
        status=ExperimentStatus.DRAFT,
        
        # ç‰ˆæœ¬è®¾ç½®
        control_version="v4_default",           # å¯¹ç…§ç»„ï¼šå½“å‰é»˜è®¤ç‰ˆæœ¬
        treatment_versions=["v4_friendly"],     # å®éªŒç»„ï¼šæ›´å‹å–„çš„ç‰ˆæœ¬
        
        # æµé‡åˆ†é…
        traffic_allocation={
            "control": 70,      # 70% ç”¨æˆ·ä½¿ç”¨é»˜è®¤ç‰ˆæœ¬
            "treatment_0": 30   # 30% ç”¨æˆ·ä½¿ç”¨å‹å–„ç‰ˆæœ¬
        },
        
        # ç›®æ ‡è®¾ç½®
        target_channels=["threema"],            # åªåœ¨ Threema æ¸ é“æµ‹è¯•
        target_user_groups=[],                  # æ‰€æœ‰ç”¨æˆ·ç»„
        exclude_users=[],                       # ä¸æ’é™¤ä»»ä½•ç”¨æˆ·
        
        # æ—¶é—´è®¾ç½®
        max_duration_hours=168,                 # æœ€å¤šè¿è¡Œ7å¤©
        
        # å®‰å…¨è®¾ç½®
        max_error_rate=0.05,                   # é”™è¯¯ç‡è¶…è¿‡5%åˆ™æš‚åœ
        min_sample_size=50,                    # è‡³å°‘50ä¸ªæ ·æœ¬
        
        # æŒ‡æ ‡è®¾ç½®
        primary_metrics=["user_rating", "response_time"],
        secondary_metrics=["clarification_rate", "tool_call_count"]
    )
    
    return config


def create_clarification_strategy_experiment():
    """ç¤ºä¾‹2: æµ‹è¯•æ¾„æ¸…ç­–ç•¥çš„ A/B å®éªŒ"""
    
    config = ExperimentConfig(
        id="clarification_strategy_v1", 
        name="æ¾„æ¸…ç­–ç•¥æµ‹è¯•ï¼šä¸»åŠ¨ vs æ™ºèƒ½æ¨æ–­",
        description="æµ‹è¯•æ›´æ™ºèƒ½çš„æ¨æ–­æ˜¯å¦èƒ½å‡å°‘æ¾„æ¸…æ¬¡æ•°å¹¶æå‡ä½“éªŒ",
        status=ExperimentStatus.DRAFT,
        
        control_version="v4_default",
        treatment_versions=["v4_smart_inference"],
        
        traffic_allocation={
            "control": 50,
            "treatment_0": 50
        },
        
        target_channels=["threema", "api"],
        
        max_duration_hours=120,  # 5å¤©
        max_error_rate=0.03,     # æ›´ä¸¥æ ¼çš„é”™è¯¯ç‡
        min_sample_size=100,
        
        primary_metrics=["clarification_rate", "success_rate"],
        secondary_metrics=["user_rating", "response_length"]
    )
    
    return config


async def setup_experiments():
    """è®¾ç½®å®éªŒ"""
    
    print("ğŸ§ª è®¾ç½® A/B æµ‹è¯•å®éªŒ...")
    
    # åˆ›å»ºå®éªŒé…ç½®
    style_exp = create_conversational_style_experiment()
    clarification_exp = create_clarification_strategy_experiment()
    
    # æ³¨å†Œå®éªŒ
    success1 = ab_manager.create_experiment(style_exp)
    success2 = ab_manager.create_experiment(clarification_exp)
    
    if success1:
        print(f"âœ… å®éªŒ '{style_exp.name}' åˆ›å»ºæˆåŠŸ")
    if success2:
        print(f"âœ… å®éªŒ '{clarification_exp.name}' åˆ›å»ºæˆåŠŸ")
    
    # å¯åŠ¨ç¬¬ä¸€ä¸ªå®éªŒ
    style_exp.status = ExperimentStatus.RUNNING
    style_exp.start_time = datetime.now().timestamp()
    ab_manager._experiments[style_exp.id] = style_exp
    
    print(f"ğŸš€ å®éªŒ '{style_exp.name}' å·²å¯åŠ¨")
    
    return success1 and success2


def simulate_user_interactions():
    """æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’å’Œå®éªŒæ•°æ®æ”¶é›†"""
    
    print("\nğŸ“Š æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’...")
    
    # æ¨¡æ‹Ÿä¸åŒç”¨æˆ·çš„äº¤äº’
    test_users = [
        {"id": "user_001", "channel": "threema"},
        {"id": "user_002", "channel": "threema"}, 
        {"id": "user_003", "channel": "api"},
        {"id": "user_004", "channel": "threema"},
        {"id": "user_005", "channel": "api"},
    ]
    
    for user in test_users:
        # è·å–ç”¨æˆ·çš„å®éªŒç‰ˆæœ¬
        version = get_experiment_version(
            user_id=user["id"],
            channel=user["channel"]
        )
        
        print(f"ğŸ‘¤ ç”¨æˆ· {user['id']} ({user['channel']}) â†’ ç‰ˆæœ¬: {version}")
        
        # æ¨¡æ‹Ÿå®éªŒç»“æœ
        variant, _ = ab_manager.get_variant_for_user(
            user["id"], 
            "conversation_style_v1", 
            channel=user["channel"]
        )
        
        if variant != "control":
            result = ExperimentResult(
                user_id=user["id"],
                experiment_id="conversation_style_v1",
                variant=variant,
                trace_id=f"trace_{user['id']}_001",
                channel=user["channel"],
                timestamp=datetime.now().timestamp(),
                response_time_ms=1200,  # æ¨¡æ‹Ÿå“åº”æ—¶é—´
                success=True,
                need_clarification=False,
                tool_calls_count=1,
                response_length=85,
                user_rating=4 if variant == "treatment_0" else 3  # å‹å–„ç‰ˆæœ¬è¯„åˆ†æ›´é«˜
            )
            
            ab_manager.record_result(result)


def analyze_experiment_results():
    """åˆ†æå®éªŒç»“æœ"""
    
    print("\nğŸ“ˆ åˆ†æå®éªŒç»“æœ...")
    
    # è·å–å®éªŒç»Ÿè®¡
    stats = ab_manager.get_experiment_stats("conversation_style_v1")
    
    print(f"\nå®éªŒ: {stats.get('name', 'Unknown')}")
    print(f"çŠ¶æ€: {stats.get('status', 'Unknown')}")
    print(f"æ€»æ ·æœ¬é‡: {stats.get('total_samples', 0)}")
    
    if stats.get('variants'):
        print("\nå„å˜é‡è¡¨ç°:")
        for variant, metrics in stats['variants'].items():
            print(f"  {variant}:")
            print(f"    æ ·æœ¬é‡: {metrics['sample_size']}")
            print(f"    æˆåŠŸç‡: {metrics['success_rate']*100:.1f}%")
            print(f"    å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time_ms']:.0f}ms")
            print(f"    æ¾„æ¸…ç‡: {metrics['clarification_rate']*100:.1f}%")


def demonstrate_safety_features():
    """æ¼”ç¤ºå®‰å…¨ç‰¹æ€§"""
    
    print("\nğŸ›¡ï¸  æ¼”ç¤ºå®‰å…¨ç‰¹æ€§...")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªæœ‰é—®é¢˜çš„å®éªŒç»“æœï¼ˆé«˜é”™è¯¯ç‡ï¼‰
    for i in range(10):
        bad_result = ExperimentResult(
            user_id=f"test_user_{i}",
            experiment_id="conversation_style_v1", 
            variant="treatment_0",
            trace_id=f"trace_bad_{i}",
            channel="threema",
            timestamp=datetime.now().timestamp(),
            response_time_ms=5000,  # å“åº”å¾ˆæ…¢
            success=False,          # å¤±è´¥
            error_type="AnalysisError"
        )
        
        ab_manager.record_result(bad_result)
    
    print("âš ï¸  æ£€æµ‹åˆ°å®éªŒç»„é”™è¯¯ç‡è¿‡é«˜ï¼Œå®éªŒå¯èƒ½ä¼šè¢«è‡ªåŠ¨æš‚åœ")
    
    # æ£€æŸ¥å®éªŒçŠ¶æ€
    experiment = ab_manager._experiments.get("conversation_style_v1")
    if experiment and experiment.status == ExperimentStatus.PAUSED:
        print("ğŸ”´ å®éªŒå·²è¢«è‡ªåŠ¨æš‚åœä»¥ä¿æŠ¤ç”¨æˆ·ä½“éªŒ")
    else:
        print("ğŸŸ¡ å®éªŒä»åœ¨è¿è¡Œä¸­ï¼ˆå®‰å…¨æ£€æŸ¥å¯èƒ½éœ€è¦æ›´å¤šæ ·æœ¬ï¼‰")


async def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤º A/B æµ‹è¯•çš„å®Œæ•´æµç¨‹"""
    
    print("ğŸ¯ FAA A/B æµ‹è¯•æ¡†æ¶æ¼”ç¤º\n")
    
    # 1. è®¾ç½®å®éªŒ
    await setup_experiments()
    
    # 2. æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’
    simulate_user_interactions()
    
    # 3. åˆ†æç»“æœ
    analyze_experiment_results()
    
    # 4. æ¼”ç¤ºå®‰å…¨ç‰¹æ€§
    demonstrate_safety_features()
    
    print("\nâœ¨ A/B æµ‹è¯•æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ’¡ å®é™…ä½¿ç”¨ä¸­ï¼Œä½ å¯ä»¥:")
    print("   1. åœ¨ prompts/family_assistant_prompts.yaml ä¸­å®šä¹‰æ–°ç‰ˆæœ¬")
    print("   2. ä½¿ç”¨æ­¤æ¡†æ¶å®‰å…¨åœ°æµ‹è¯•æ–°ç‰ˆæœ¬")
    print("   3. åŸºäºæ•°æ®å†³å®šæ˜¯å¦é‡‡ç”¨æ–°ç‰ˆæœ¬")


if __name__ == "__main__":
    asyncio.run(main())
